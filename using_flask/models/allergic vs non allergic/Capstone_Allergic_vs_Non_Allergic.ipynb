{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D9HFraDGPSPk"
      },
      "outputs": [],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJuUpgQCRR1d",
        "outputId": "8261ba0f-bd93-4528-ad52-1e847f4af4a2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# import os\n",
        "\n",
        "# # Specify the paths to your original images and the directory to save augmented images\n",
        "# original_images_path = '/content/drive/MyDrive/Capstone/allergic vs non_allergic/hypersensitivity_allergic_dermatosis'\n",
        "# augmented_images_path = '/content/allergic_augmented'\n",
        "\n",
        "# # Create the directory to save augmented images if it doesn't exist\n",
        "# os.makedirs(augmented_images_path, exist_ok=True)\n",
        "\n",
        "# # Specify the augmentation parameters\n",
        "# rotation_angles = [30, 60, 90, 120, 150]  # Rotation angles\n",
        "# flipping_codes = [0, 1, -1]  # Flipping codes\n",
        "\n",
        "# # Iterate through each original image\n",
        "# for filename in os.listdir(original_images_path):\n",
        "#     if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust the file extension as needed\n",
        "#         # Load the original image\n",
        "#         image_path = os.path.join(original_images_path, filename)\n",
        "#         image = cv2.imread(image_path)\n",
        "\n",
        "#         # Generate rotated variations\n",
        "#         for angle in rotation_angles:\n",
        "#             # Rotate the image\n",
        "#             rotation_matrix = cv2.getRotationMatrix2D((image.shape[1] // 2, image.shape[0] // 2), angle, 1.0)\n",
        "#             rotated_image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
        "\n",
        "#             # Save the rotated image\n",
        "#             save_path = os.path.join(augmented_images_path, f'rotated_{angle}_{filename}')\n",
        "#             cv2.imwrite(save_path, rotated_image)\n",
        "\n",
        "#         # Generate flipped variations\n",
        "#         for code in flipping_codes:\n",
        "#             # Flip the image\n",
        "#             flipped_image = cv2.flip(image, code)\n",
        "\n",
        "#             # Save the flipped image\n",
        "#             flip_code_str = 'h' if code == 0 else 'v' if code == 1 else 'hv'\n",
        "#             save_path = os.path.join(augmented_images_path, f'flipped_{flip_code_str}_{filename}')\n",
        "#             cv2.imwrite(save_path, flipped_image)\n"
      ],
      "metadata": {
        "id": "JUWD4_dJWIrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# Specify the paths to your original images and the directory to save augmented images\n",
        "original_images_path = '/content/drive/MyDrive/Capstone/allergic vs non_allergic/hypersensitivity_allergic_dermatosis'\n",
        "augmented_images_path = '/content/allergic_augmented_new'\n",
        "\n",
        "# Create the directory to save augmented images if it doesn't exist\n",
        "os.makedirs(augmented_images_path, exist_ok=True)\n",
        "\n",
        "# Specify the augmentation parameters\n",
        "rotation_angles = [30, 60, 90, 120, 150]  # Rotation angles\n",
        "flipping_codes = [0, 1, -1]  # Flipping codes\n",
        "brightness_factors = [0.6, 1.2]  # Brightness adjustment factors\n",
        "saturation_factors = [0.6, 1.2]  # Saturation adjustment factors\n",
        "\n",
        "# Iterate through each original image\n",
        "for filename in os.listdir(original_images_path):\n",
        "    if filename.endswith('.jpg') or filename.endswith('.png'):  # Adjust the file extension as needed\n",
        "        # Load the original image\n",
        "        image_path = os.path.join(original_images_path, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "\n",
        "        # Generate rotated variations\n",
        "        for angle in rotation_angles:\n",
        "            # Rotate the image\n",
        "            rotation_matrix = cv2.getRotationMatrix2D((image.shape[1] // 2, image.shape[0] // 2), angle, 1.0)\n",
        "            rotated_image = cv2.warpAffine(image, rotation_matrix, (image.shape[1], image.shape[0]))\n",
        "\n",
        "            # Save the rotated image\n",
        "            save_path = os.path.join(augmented_images_path, f'rotated_{angle}_{filename}')\n",
        "            cv2.imwrite(save_path, rotated_image)\n",
        "\n",
        "        # Generate flipped variations\n",
        "        for code in flipping_codes:\n",
        "            # Flip the image\n",
        "            flipped_image = cv2.flip(image, code)\n",
        "\n",
        "            # Save the flipped image\n",
        "            flip_code_str = 'h' if code == 0 else 'v' if code == 1 else 'hv'\n",
        "            save_path = os.path.join(augmented_images_path, f'flipped_{flip_code_str}_{filename}')\n",
        "            cv2.imwrite(save_path, flipped_image)\n",
        "\n",
        "        # Generate brightness variations\n",
        "        for factor in brightness_factors:\n",
        "            # Adjust the image brightness\n",
        "            brightened_image = cv2.convertScaleAbs(image, alpha=factor, beta=0)\n",
        "            darkened_image = cv2.convertScaleAbs(image, alpha=1.0, beta=-50)\n",
        "\n",
        "            # Save the brightened and darkened images\n",
        "            save_path_brightened = os.path.join(augmented_images_path, f'brightened_{int(factor*10)}_{filename}')\n",
        "            save_path_darkened = os.path.join(augmented_images_path, f'darkened_{int(factor*10)}_{filename}')\n",
        "            cv2.imwrite(save_path_brightened, brightened_image)\n",
        "            cv2.imwrite(save_path_darkened, darkened_image)\n",
        "\n",
        "        # Generate saturation variations\n",
        "        for factor in saturation_factors:\n",
        "            # Convert the image to HSV color space\n",
        "            hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "            # Scale the saturation channel\n",
        "            hsv_image[:, :, 1] = np.clip(hsv_image[:, :, 1] * factor, 0, 255)\n",
        "\n",
        "            # Convert the image back to BGR color space\n"
      ],
      "metadata": {
        "id": "9nA2ZZ3Lbzaa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "source_path = '/content/drive/MyDrive/Capstone/allergic vs non_allergic'\n",
        "\n",
        "source_path_fungal = os.path.join(source_path, 'hypersensitivity_allergic_dermatosis')\n",
        "source_path_non_fungal = os.path.join(source_path, 'non_allergic')\n",
        "\n",
        "# os.listdir returns a list containing all files under the given path\n",
        "print(f\"There are {len(os.listdir(source_path_fungal))} images of Fungal Infection.\")\n",
        "print(f\"There are {len(os.listdir(source_path_non_fungal))} images of Non Fungal\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT-ZOxkQYPON",
        "outputId": "d3841ac4-2f2a-410f-f29c-aecbcaca76b5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 13 images of Fungal Infection.\n",
            "There are 49 images of Non Fungal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Define root directory\n",
        "root_dir = '/content/allergic vs non'\n",
        "\n",
        "# Empty directory to prevent FileExistsError is the function is run several times\n",
        "if os.path.exists(root_dir):\n",
        "  shutil.rmtree(root_dir)\n",
        "\n",
        "# GRADED FUNCTION: create_train_val_dirs\n",
        "def create_train_val_dirs(root_path):\n",
        "  \"\"\"\n",
        "  Creates directories for the train and test sets\n",
        "  \n",
        "  Args:\n",
        "    root_path (string) - the base directory path to create subdirectories from\n",
        "  \n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "  ### START CODE HERE\n",
        "  os.makedirs(root_path)\n",
        "  train_dir = os.path.join(root_path, 'training')\n",
        "  val_dir = os.path.join(root_path, 'validation')\n",
        "\n",
        "  # HINT:\n",
        "  # Use os.makedirs to create your directories with intermediate subdirectories\n",
        "  # Don't hardcode the paths. Use os.path.join to append the new directories to the root_path parameter\n",
        "\n",
        "  train_cats_dir = os.path.join(train_dir, 'allergic')\n",
        "  train_dogs_dir = os.path.join(train_dir, 'nonallergic')\n",
        "  val_cats_dir = os.path.join(val_dir, 'allergic')\n",
        "  val_dogs_dir = os.path.join(val_dir, 'nonallergic')\n",
        "\n",
        "  os.makedirs(train_cats_dir)\n",
        "  os.makedirs(train_dogs_dir)\n",
        "  os.makedirs(val_cats_dir)\n",
        "  os.makedirs(val_dogs_dir)\n",
        "\n",
        "  pass\n",
        "  \n",
        "\n",
        "  ### END CODE HERE\n",
        "\n",
        "  \n",
        "try:\n",
        "  create_train_val_dirs(root_path=root_dir)\n",
        "except FileExistsError:\n",
        "  print(\"You should not be seeing this since the upper directory is removed beforehand\")"
      ],
      "metadata": {
        "id": "1PAkT7ZPTOhA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Test your create_train_val_dirs function\n",
        "\n",
        "for rootdir, dirs, files in os.walk(root_dir):\n",
        "    for subdir in dirs:\n",
        "        print(os.path.join(rootdir, subdir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60NkMtn-TtSH",
        "outputId": "403c233e-5e35-4d11-bd0d-c289ccdb79f8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/allergic vs non/training\n",
            "/content/allergic vs non/validation\n",
            "/content/allergic vs non/training/allergic\n",
            "/content/allergic vs non/training/nonallergic\n",
            "/content/allergic vs non/validation/allergic\n",
            "/content/allergic vs non/validation/nonallergic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # function to crop image\n",
        "# # note: the amount of croped pixels were approxed by me (nopal) \n",
        "# #       by measure the excess pixels\n",
        "# def crop_image(image):\n",
        "#     crop_width = 1920 - 170 - 160  # Calculate the resulting width after trimming\n",
        "#     crop_height = 1080  # Height remains the same\n",
        "#     crop_location = (170, 0)  # Starting position of the crop\n",
        "#     cropped_image = tf.image.crop(image, crop_location, [crop_height, crop_width])\n",
        "#     return cropped_image"
      ],
      "metadata": {
        "id": "p-M8YyTUZc21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def crop_image(image):\n",
        "#     crop_width = 1920 - 170 - 150  # Calculate the resulting width after trimming\n",
        "#     crop_height = 1080  # Height remains the same\n",
        "#     crop_location = (170, 0)  # Starting position of the crop\n",
        "\n",
        "#     # Crop the image using array indexing\n",
        "#     cropped_image = image[crop_location[1]:crop_location[1] + crop_height,\n",
        "#                           crop_location[0]:crop_location[0] + crop_width, :]\n",
        "\n",
        "#     return cropped_image"
      ],
      "metadata": {
        "id": "V18qe6fFZhGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # function to split dataset into training and validation\n",
        "# def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
        "#     files = os.listdir(SOURCE_DIR)\n",
        "\n",
        "#     verified_files = []\n",
        "#     for file in files:\n",
        "#         if os.path.getsize(os.path.join(SOURCE_DIR, file)) == 0:\n",
        "#             print(f\"{file} is zero length, so ignoring.\")\n",
        "#         else:\n",
        "#             verified_files.append(file)\n",
        "\n",
        "#     # Shuffle the files\n",
        "#     random.shuffle(verified_files)\n",
        "\n",
        "#     # Calculate the split index\n",
        "#     split_idx = int(SPLIT_SIZE * len(verified_files))\n",
        "\n",
        "#     # Split the files\n",
        "#     train_files = verified_files[:split_idx]\n",
        "#     val_files = verified_files[split_idx:]\n",
        "\n",
        "#     # Copy train files\n",
        "#     for file in train_files:\n",
        "#         src_path = os.path.join(SOURCE_DIR, file)\n",
        "#         dst_path = os.path.join(TRAINING_DIR, file)\n",
        "#         # Load the image using TensorFlow\n",
        "#         image = tf.io.read_file(src_path)\n",
        "#         image = tf.image.decode_image(image)\n",
        "#         # Crop the image\n",
        "#         cropped_image = crop_image(image)\n",
        "#         # Save the cropped image to the destination directory\n",
        "#         tf.io.write_file(dst_path, tf.image.encode_png(cropped_image))\n",
        "\n",
        "#     # Copy validation files\n",
        "#     for file in val_files:\n",
        "#         src_path = os.path.join(SOURCE_DIR, file)\n",
        "#         dst_path = os.path.join(VALIDATION_DIR, file)\n",
        "#         # Load the image using TensorFlow\n",
        "#         image = tf.io.read_file(src_path)\n",
        "#         image = tf.image.decode_image(image)\n",
        "#         # Crop the image\n",
        "#         cropped_image = crop_image(image)\n",
        "#         # Save the cropped image to the destination directory\n",
        "#         tf.io.write_file(dst_path, tf.image.encode_png(cropped_image))"
      ],
      "metadata": {
        "id": "r9fFDRIIZtrj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # grader-required-cell\n",
        "\n",
        "# # GRADED FUNCTION: split_data\n",
        "# def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
        "#   \"\"\"\n",
        "#   Splits the data into train and test sets\n",
        "  \n",
        "#   Args:\n",
        "#     SOURCE_DIR (string): directory path containing the images\n",
        "#     TRAINING_DIR (string): directory path to be used for training\n",
        "#     VALIDATION_DIR (string): directory path to be used for validation\n",
        "#     SPLIT_SIZE (float): proportion of the dataset to be used for training\n",
        "    \n",
        "#   Returns:\n",
        "#     None\n",
        "#   \"\"\"\n",
        "\n",
        "#   ### START CODE HERE\n",
        "#   file_list = []\n",
        "\n",
        "\n",
        "#   for file in os.listdir(SOURCE_DIR):\n",
        "#     file_path = os.path.join(SOURCE_DIR,file)\n",
        "#     if(os.path.getsize(file_path) == 0):\n",
        "#       print(file + \" is zero length, so ignoring.\")\n",
        "#     else:\n",
        "#       file_list.append(file_path)\n",
        "\n",
        "#   train_num_items = int(round(len(file_list) * SPLIT_SIZE, 0))\n",
        "#   train_list = random.sample(file_list, train_num_items)\n",
        "#   val_list = list(set(file_list) - set(train_list))\n",
        "\n",
        "#   for f in train_list:\n",
        "#     copyfile(f,TRAINING_DIR+os.path.basename(f))\n",
        "#   for f in val_list:\n",
        "#     copyfile(f,VALIDATION_DIR+os.path.basename(f))\n",
        "\n",
        "#   pass\n",
        "\n",
        "#   ### END CODE HERE"
      ],
      "metadata": {
        "id": "9oHwLZUxT3C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
        "  \"\"\"\n",
        "  Splits the data into train and test sets\n",
        "  \n",
        "  Args:\n",
        "    SOURCE_DIR (string): directory path containing the images\n",
        "    TRAINING_DIR (string): directory path to be used for training\n",
        "    VALIDATION_DIR (string): directory path to be used for validation\n",
        "    SPLIT_SIZE (float): proportion of the dataset to be used for training\n",
        "    \n",
        "  Returns:\n",
        "    None\n",
        "  \"\"\"\n",
        "\n",
        "  file_list = []\n",
        "\n",
        "  for file in os.listdir(SOURCE_DIR):\n",
        "    file_path = os.path.join(SOURCE_DIR, file)\n",
        "    if os.path.getsize(file_path) == 0:\n",
        "      print(file + \" is zero length, so ignoring.\")\n",
        "    else:\n",
        "      file_list.append(file_path)\n",
        "\n",
        "  random.shuffle(file_list)  # Shuffle the file list randomly\n",
        "\n",
        "  train_num_items = int(round(len(file_list) * SPLIT_SIZE, 0))\n",
        "  train_list = file_list[:train_num_items]\n",
        "  val_list = file_list[train_num_items:]\n",
        "\n",
        "  for f in train_list:\n",
        "    copyfile(f, TRAINING_DIR + os.path.basename(f))\n",
        "  for f in val_list:\n",
        "    copyfile(f, VALIDATION_DIR + os.path.basename(f))\n",
        "\n",
        "  pass"
      ],
      "metadata": {
        "id": "9nYtNZrEEtrb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Test your split_data function\n",
        "\n",
        "# Define paths\n",
        "CAT_SOURCE_DIR = \"/content/allergic_augmented_new\"\n",
        "DOG_SOURCE_DIR = \"/content/drive/MyDrive/Capstone/allergic vs non_allergic/non_allergic\"\n",
        "\n",
        "TRAINING_DIR = \"/content/allergic vs non/training\"\n",
        "VALIDATION_DIR = \"/content/allergic vs non/validation\"\n",
        "\n",
        "TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"allergic/\")\n",
        "VALIDATION_CATS_DIR = os.path.join(VALIDATION_DIR, \"allergic/\")\n",
        "\n",
        "TRAINING_DOGS_DIR = os.path.join(TRAINING_DIR, \"nonallergic/\")\n",
        "VALIDATION_DOGS_DIR = os.path.join(VALIDATION_DIR, \"nonallergic/\")\n",
        "\n",
        "# Empty directories in case you run this cell multiple times\n",
        "if len(os.listdir(TRAINING_CATS_DIR)) > 0:\n",
        "  for file in os.scandir(TRAINING_CATS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TRAINING_DOGS_DIR)) > 0:\n",
        "  for file in os.scandir(TRAINING_DOGS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(VALIDATION_CATS_DIR)) > 0:\n",
        "  for file in os.scandir(VALIDATION_CATS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(VALIDATION_DOGS_DIR)) > 0:\n",
        "  for file in os.scandir(VALIDATION_DOGS_DIR):\n",
        "    os.remove(file.path)\n",
        "\n",
        "# Define proportion of images used for training\n",
        "split_size = .8\n",
        "\n",
        "# Run the function\n",
        "# NOTE: Messages about zero length images should be printed out\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, VALIDATION_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, VALIDATION_DOGS_DIR, split_size)\n",
        "\n",
        "# Check that the number of images matches the expected output\n",
        "\n",
        "# Your function should perform copies rather than moving images so original directories should contain unchanged images\n",
        "print(f\"\\n\\nOriginal allergic directory has {len(os.listdir(CAT_SOURCE_DIR))} images\")\n",
        "print(f\"Original nonallergic directory has {len(DOG_SOURCE_DIR)} images\\n\")\n",
        "\n",
        "# Training and validation splits\n",
        "print(f\"There are {len(os.listdir(TRAINING_CATS_DIR))} images of allergic for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_DOGS_DIR))} images of nonallergic for training\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_CATS_DIR))} images of allergic for validation\")\n",
        "print(f\"There are {len(os.listdir(VALIDATION_DOGS_DIR))} images of nonallergic for validation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taJ9ykn9T5JX",
        "outputId": "5a94d6f3-1161-48ae-b015-21ff393562f4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Original allergic directory has 156 images\n",
            "Original nonallergic directory has 69 images\n",
            "\n",
            "There are 125 images of allergic for training\n",
            "There are 39 images of nonallergic for training\n",
            "There are 31 images of allergic for validation\n",
            "There are 10 images of nonallergic for validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# GRADED FUNCTION: train_val_generators\n",
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "  \"\"\"\n",
        "  Creates the training and validation data generators\n",
        "  \n",
        "  Args:\n",
        "    TRAINING_DIR (string): directory path containing the training images\n",
        "    VALIDATION_DIR (string): directory path containing the testing/validation images\n",
        "    \n",
        "  Returns:\n",
        "    train_generator, validation_generator - tuple containing the generators\n",
        "  \"\"\"\n",
        "  ### START CODE HERE\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
        "  train_datagen = ImageDataGenerator(    \n",
        "    rescale=1.0/255.)\n",
        "    # rotation_range=10,\n",
        "    # width_shift_range=0.1,\n",
        "    # height_shift_range=0.1,\n",
        "    # shear_range=0.1,\n",
        "    # zoom_range=0.1,\n",
        "    # horizontal_flip=True,\n",
        "    # fill_mode='nearest')\n",
        "    # rescale=1.0/255,\n",
        "    # featurewise_center=True,\n",
        "    # featurewise_std_normalization=True,\n",
        "    # rotation_range=20,\n",
        "    # width_shift_range=0.2,\n",
        "    # height_shift_range=0.2,\n",
        "    # horizontal_flip=True,)\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
        "                                                      batch_size=16,\n",
        "                                                      class_mode='binary',\n",
        "                                                      target_size=(128, 128))\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
        "  validation_datagen = ImageDataGenerator(\n",
        "    rescale=1.0/255.)\n",
        "    # rotation_range=10,\n",
        "    # width_shift_range=0.1,\n",
        "    # height_shift_range=0.1,\n",
        "    # shear_range=0.1,\n",
        "    # zoom_range=0.1,\n",
        "    # horizontal_flip=True,\n",
        "    # fill_mode='nearest')\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
        "                                                                batch_size=16,\n",
        "                                                                class_mode='binary',\n",
        "                                                                target_size=(128, 128))\n",
        "  ### END CODE HERE\n",
        "  return train_generator, validation_generator"
      ],
      "metadata": {
        "id": "sBZN_JC_UnKM"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Test your generators\n",
        "train_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPl2qpS4Uqkv",
        "outputId": "9dcc9c20-0d0d-4fc1-9a87-d14254335de0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 164 images belonging to 2 classes.\n",
            "Found 41 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ba844313",
        "tags": [
          "graded"
        ]
      },
      "outputs": [],
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# Define a Callback class that stops training once accuracy reaches 99.9%\n",
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if logs.get('accuracy') > 0.9 and logs.get('val_accuracy') > 0.86:\n",
        "      print(\"\\nReached 99.9% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# GRADED FUNCTION: create_model\n",
        "def create_model():\n",
        "  # DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
        "  # USE AT LEAST 3 CONVOLUTION LAYERS\n",
        "\n",
        "  ### START CODE HERE\n",
        "\n",
        "  model = tf.keras.models.Sequential([ \n",
        "      tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2), \n",
        "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      tf.keras.layers.Conv2D(128, (3,3), activation='relu'), \n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      # Flatten the results to feed into a DNN\n",
        "      tf.keras.layers.Flatten(), \n",
        "      # 512 neuron hidden layer\n",
        "      tf.keras.layers.Dense(512, activation='relu'), \n",
        "      # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  \n",
        "  model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy']) \n",
        "    \n",
        "  ### END CODE HERE\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "cxOHZQipUuvE"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRDa7-8I7Jhu",
        "outputId": "ac60a005-0eaf-4373-f629-3dbf72992700"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 126, 126, 16)      448       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 63, 63, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 61, 61, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 30, 30, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 28, 28, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 14, 14, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_3 (Conv2D)           (None, 12, 12, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_3 (MaxPooling  (None, 6, 6, 128)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4608)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               2359808   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 513       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,457,761\n",
            "Trainable params: 2,457,761\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this and see how many epochs it should take before the callback\n",
        "# fires, and stops training at 99.9% accuracy\n",
        "# (It should take a few epochs)\n",
        "model = create_model()\n",
        "callbacks = myCallback()\n",
        "history = model.fit(train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    epochs = 50,\n",
        "                    verbose = 1,\n",
        "                    callbacks=callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-esmAoZ33c7",
        "outputId": "b0b69c4a-7551-4f20-9cba-b86d51303b1b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "11/11 [==============================] - 21s 859ms/step - loss: 0.7321 - accuracy: 0.7134 - val_loss: 0.6070 - val_accuracy: 0.7561\n",
            "Epoch 2/50\n",
            "11/11 [==============================] - 8s 703ms/step - loss: 0.5465 - accuracy: 0.7622 - val_loss: 0.4901 - val_accuracy: 0.7561\n",
            "Epoch 3/50\n",
            "11/11 [==============================] - 9s 865ms/step - loss: 0.4236 - accuracy: 0.7622 - val_loss: 0.3891 - val_accuracy: 0.7561\n",
            "Epoch 4/50\n",
            "11/11 [==============================] - 8s 710ms/step - loss: 0.3556 - accuracy: 0.8354 - val_loss: 0.4186 - val_accuracy: 0.7561\n",
            "Epoch 5/50\n",
            "11/11 [==============================] - 7s 672ms/step - loss: 0.3679 - accuracy: 0.8171 - val_loss: 0.3729 - val_accuracy: 0.7805\n",
            "Epoch 6/50\n",
            "11/11 [==============================] - 9s 814ms/step - loss: 0.3706 - accuracy: 0.8415 - val_loss: 0.4128 - val_accuracy: 0.7805\n",
            "Epoch 7/50\n",
            "11/11 [==============================] - 9s 803ms/step - loss: 0.3941 - accuracy: 0.7866 - val_loss: 0.4330 - val_accuracy: 0.7317\n",
            "Epoch 8/50\n",
            "11/11 [==============================] - 7s 676ms/step - loss: 0.3178 - accuracy: 0.8598 - val_loss: 0.3462 - val_accuracy: 0.8293\n",
            "Epoch 9/50\n",
            "11/11 [==============================] - 9s 762ms/step - loss: 0.2565 - accuracy: 0.8841 - val_loss: 0.3208 - val_accuracy: 0.8293\n",
            "Epoch 10/50\n",
            "11/11 [==============================] - 9s 813ms/step - loss: 0.2175 - accuracy: 0.9268 - val_loss: 0.3626 - val_accuracy: 0.8537\n",
            "Epoch 11/50\n",
            "11/11 [==============================] - 7s 662ms/step - loss: 0.2264 - accuracy: 0.8963 - val_loss: 0.3875 - val_accuracy: 0.8293\n",
            "Epoch 12/50\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1733 - accuracy: 0.9390\n",
            "Reached 99.9% accuracy so cancelling training!\n",
            "11/11 [==============================] - 9s 803ms/step - loss: 0.1733 - accuracy: 0.9390 - val_loss: 0.3425 - val_accuracy: 0.8780\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the training and validation accuracies for each epoch\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "5PHrd-Sq8g8N",
        "outputId": "4fc9b822-0be4-4ff1-e1a2-e9a91890351f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image  # Import from tf.keras instead of keras\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  # Predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(128, 128))  # Use tf.keras.preprocessing.image.load_img\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(fn)\n",
        "  print(classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "-AoDggmtZF7-",
        "outputId": "b4a5cfe6-1416-44d1-e9f6-415320726a84"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-27b8e005-dbbe-4db1-843c-235814f150ca\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-27b8e005-dbbe-4db1-843c-235814f150ca\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dog210422_04_02_33_pic0.jpg to dog210422_04_02_33_pic0.jpg\n",
            "Saving dog210505_01_02_27_pic0.jpg to dog210505_01_02_27_pic0.jpg\n",
            "Saving dog210508_06_02_20_pic0.jpg to dog210508_06_02_20_pic0.jpg\n",
            "Saving dog210612_48_02_12_pic0_aler.jpg to dog210612_48_02_12_pic0_aler.jpg\n",
            "Saving Dog210621_13_02_42_pic0.jpg to Dog210621_13_02_42_pic0.jpg\n",
            "Saving Dog210622_01_01_39_pic0_aler.jpg to Dog210622_01_01_39_pic0_aler.jpg\n",
            "Saving Dog210624_12_02_32_pic0_aler.jpg to Dog210624_12_02_32_pic0_aler.jpg\n",
            "1/1 [==============================] - 0s 235ms/step\n",
            "dog210422_04_02_33_pic0.jpg\n",
            "[[1.]]\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "dog210505_01_02_27_pic0.jpg\n",
            "[[1.]]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "dog210508_06_02_20_pic0.jpg\n",
            "[[1.]]\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "dog210612_48_02_12_pic0_aler.jpg\n",
            "[[0.]]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Dog210621_13_02_42_pic0.jpg\n",
            "[[0.]]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Dog210622_01_01_39_pic0_aler.jpg\n",
            "[[0.]]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Dog210624_12_02_32_pic0_aler.jpg\n",
            "[[0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = ['allergic', 'non-allergic']\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    # Predicting images\n",
        "    path = fn\n",
        "    img = image.load_img(path, target_size=(128, 128))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    images = np.vstack([x])\n",
        "    classes = model.predict(images, batch_size=10)\n",
        "    predicted_label = class_labels[int(classes[0][0])]\n",
        "    \n",
        "    print(fn)\n",
        "    print(predicted_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRfLI2y8Zwrk",
        "outputId": "472c675c-01cb-4b74-b328-007ee5d09dea"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "dog210422_04_02_33_pic0.jpg\n",
            "non-allergic\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "dog210505_01_02_27_pic0.jpg\n",
            "non-allergic\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "dog210508_06_02_20_pic0.jpg\n",
            "non-allergic\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "dog210612_48_02_12_pic0_aler.jpg\n",
            "allergic\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Dog210621_13_02_42_pic0.jpg\n",
            "allergic\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "Dog210622_01_01_39_pic0_aler.jpg\n",
            "allergic\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Dog210624_12_02_32_pic0_aler.jpg\n",
            "allergic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE: Save the trained model as a Keras HDF5 file. \n",
        "\n",
        "saved_model_path = \"./my_model.h5\"\n",
        "model.save(saved_model_path)\n",
        "\n",
        "# YOUR CODE HERE"
      ],
      "metadata": {
        "id": "qadL93Qt86EP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.2\n",
        "!pip install tensorflowjs\n",
        "import tensorflow as tf\n",
        "\n",
        "print('\\u2022 Using TensorFlow Version:', tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zKrerMAxHjYr",
        "outputId": "b3be9919-c67d-43c5-e4cc-b3d7d5986259"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.2 (from versions: 2.8.0rc0, 2.8.0rc1, 2.8.0, 2.8.1, 2.8.2, 2.8.3, 2.8.4, 2.9.0rc0, 2.9.0rc1, 2.9.0rc2, 2.9.0, 2.9.1, 2.9.2, 2.9.3, 2.10.0rc0, 2.10.0rc1, 2.10.0rc2, 2.10.0rc3, 2.10.0, 2.10.1, 2.11.0rc0, 2.11.0rc1, 2.11.0rc2, 2.11.0, 2.11.1, 2.12.0rc0, 2.12.0rc1, 2.12.0, 2.13.0rc0, 2.13.0rc1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.2\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorflowjs\n",
            "  Downloading tensorflowjs-4.7.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m85.1/85.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting flax<0.6.3,>=0.6.2 (from tensorflowjs)\n",
            "  Downloading flax-0.6.2-py3-none-any.whl (189 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m189.9/189.9 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (5.12.0)\n",
            "Requirement already satisfied: jax>=0.3.16 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.4.10)\n",
            "Requirement already satisfied: tensorflow<3,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (2.12.0)\n",
            "Collecting tensorflow-decision-forests>=1.3.0 (from tensorflowjs)\n",
            "  Downloading tensorflow_decision_forests-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six<2,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflowjs) (0.13.0)\n",
            "Collecting packaging~=20.9 (from tensorflowjs)\n",
            "  Downloading packaging-20.9-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (1.22.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (3.7.1)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (1.0.5)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (0.1.5)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (0.1.36)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (13.3.4)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (4.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from flax<0.6.3,>=0.6.2->tensorflowjs) (6.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.16->tensorflowjs) (0.1.0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.16->tensorflowjs) (3.3.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.16->tensorflowjs) (1.10.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from packaging~=20.9->tensorflowjs) (3.0.9)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (3.8.0)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (16.0.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (67.7.2)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<3,>=2.12.0->tensorflowjs) (0.32.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.3.0->tensorflowjs) (1.5.3)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from tensorflow-decision-forests>=1.3.0->tensorflowjs) (0.40.0)\n",
            "Collecting wurlitzer (from tensorflow-decision-forests>=1.3.0->tensorflowjs)\n",
            "  Downloading wurlitzer-3.0.3-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax<0.6.3,>=0.6.2->tensorflowjs) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1->flax<0.6.3,>=0.6.2->tensorflowjs) (2.14.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (8.4.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->flax<0.6.3,>=0.6.2->tensorflowjs) (2.8.2)\n",
            "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from optax->flax<0.6.3,>=0.6.2->tensorflowjs) (0.1.7)\n",
            "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.10/dist-packages (from optax->flax<0.6.3,>=0.6.2->tensorflowjs) (0.4.10+cuda11.cudnn86)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->tensorflow-decision-forests>=1.3.0->tensorflowjs) (2022.7.1)\n",
            "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax<0.6.3,>=0.6.2->tensorflowjs) (0.1.8)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from chex>=0.1.5->optax->flax<0.6.3,>=0.6.2->tensorflowjs) (0.12.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (1.3.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=11.1->flax<0.6.3,>=0.6.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow<3,>=2.12.0->tensorflowjs) (3.2.2)\n",
            "Installing collected packages: wurlitzer, packaging, flax, tensorflow-decision-forests, tensorflowjs\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 23.1\n",
            "    Uninstalling packaging-23.1:\n",
            "      Successfully uninstalled packaging-23.1\n",
            "  Attempting uninstall: flax\n",
            "    Found existing installation: flax 0.6.9\n",
            "    Uninstalling flax-0.6.9:\n",
            "      Successfully uninstalled flax-0.6.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "statsmodels 0.13.5 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\n",
            "xarray 2022.12.0 requires packaging>=21.3, but you have packaging 20.9 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed flax-0.6.2 packaging-20.9 tensorflow-decision-forests-1.3.0 tensorflowjs-4.7.0 wurlitzer-3.0.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "packaging"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Using TensorFlow Version: 2.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXERCISE: Use the tensorflow.js converter to convert the saved Keras model into JSON format.\n",
        "# YOUR CODE HERE\n",
        "!tensorflowjs_converter --input_format=keras {saved_model_path} ./"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nybrj_K89EKA",
        "outputId": "0b1c5cbe-1977-418b-fc93-48b937d93c1c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-12 04:59:21.202285: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import zipfile\n",
        "\n",
        "# # Specify the filenames\n",
        "# h5_file = \"./my_model.h5\"\n",
        "# json_file = \"./model.json\"\n",
        "\n",
        "# # Create a zip file\n",
        "# zip_file_name = \"Fungal vs Non Fungal.zip\"\n",
        "# with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "#     # Add the h5 file to the zip\n",
        "#     zipf.write(h5_file, arcname='my_model.h5')\n",
        "\n",
        "#     # Add the json file to the zip\n",
        "#     zipf.write(json_file, arcname='model.json')\n",
        "\n",
        "# print(\"Output files zipped successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jcuvzyseBV0F",
        "outputId": "cae5c16a-1f68-4583-c3f2-695c6526b256"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output files zipped successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "# Specify the filenames\n",
        "h5_file = \"./my_model.h5\"\n",
        "json_file = \"./model.json\"\n",
        "bin_file1 = \"./group1-shard1of3.bin\"\n",
        "bin_file2 = \"./group1-shard2of3.bin\"\n",
        "bin_file3 = \"./group1-shard3of3.bin\"\n",
        "\n",
        "# Create a zip file\n",
        "zip_file_name = \"allergic vs non allergic.zip\"\n",
        "with zipfile.ZipFile(zip_file_name, 'w') as zipf:\n",
        "    # Add the h5 file to the zip\n",
        "    zipf.write(h5_file, arcname='my_model.h5')\n",
        "\n",
        "    # Add the json file to the zip\n",
        "    zipf.write(json_file, arcname='model.json')\n",
        "    \n",
        "    # Add the bin files to the zip\n",
        "    zipf.write(bin_file1, arcname='group1-shard1of3.bin')\n",
        "    zipf.write(bin_file2, arcname='group1-shard2of3.bin')\n",
        "    zipf.write(bin_file3, arcname='group1-shard3of3.bin')\n",
        "\n",
        "print(\"Output files zipped successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwR0JcJrIS5g",
        "outputId": "4b51c06d-e8d1-4769-8a34-c1be05a22af1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output files zipped successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grader-required-cell\n",
        "\n",
        "# GRADED FUNCTION: create_model\n",
        "def create_model():\n",
        "  # DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
        "  # USE AT LEAST 3 CONVOLUTION LAYERS\n",
        "\n",
        "  ### START CODE HERE\n",
        "\n",
        "  model = tf.keras.models.Sequential([ \n",
        "      tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "      tf.keras.layers.MaxPooling2D(2,2), \n",
        "      tf.keras.layers.Conv2D(64, (3,3), activation='relu'), \n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      tf.keras.layers.Conv2D(128, (3,3), activation='relu'), \n",
        "      tf.keras.layers.MaxPooling2D(2,2),\n",
        "      # Flatten the results to feed into a DNN\n",
        "      tf.keras.layers.Flatten(), \n",
        "      # 512 neuron hidden layer\n",
        "      tf.keras.layers.Dense(512, activation='relu'), \n",
        "      # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('cats') and 1 for the other ('dogs')\n",
        "      tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "  ])\n",
        "\n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy']) \n",
        "    \n",
        "  ### END CODE HERE\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "68U-dL_HERVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the untrained model\n",
        "model = create_model()\n",
        "\n",
        "# Train the model\n",
        "# Note that this may take some time.\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=15,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kRXSZ4WpE5EW",
        "outputId": "a197032d-9ea8-4c1b-ef1f-5d063c6a27e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "8/8 [==============================] - 11s 885ms/step - loss: 0.6463 - accuracy: 0.5984 - val_loss: 0.5623 - val_accuracy: 0.6774\n",
            "Epoch 2/15\n",
            "8/8 [==============================] - 15s 2s/step - loss: 0.5799 - accuracy: 0.6885 - val_loss: 0.4804 - val_accuracy: 0.7097\n",
            "Epoch 3/15\n",
            "8/8 [==============================] - 7s 921ms/step - loss: 0.4641 - accuracy: 0.7049 - val_loss: 0.3485 - val_accuracy: 0.8387\n",
            "Epoch 4/15\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.3779 - accuracy: 0.8197 - val_loss: 0.3667 - val_accuracy: 0.8065\n",
            "Epoch 5/15\n",
            "8/8 [==============================] - 7s 864ms/step - loss: 0.3050 - accuracy: 0.8525 - val_loss: 0.1275 - val_accuracy: 0.9677\n",
            "Epoch 6/15\n",
            "8/8 [==============================] - 7s 942ms/step - loss: 0.2620 - accuracy: 0.8934 - val_loss: 0.1106 - val_accuracy: 0.9677\n",
            "Epoch 7/15\n",
            "8/8 [==============================] - 7s 837ms/step - loss: 0.5323 - accuracy: 0.7705 - val_loss: 0.2240 - val_accuracy: 0.9677\n",
            "Epoch 8/15\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.2508 - accuracy: 0.8934 - val_loss: 0.1648 - val_accuracy: 0.9355\n",
            "Epoch 9/15\n",
            "8/8 [==============================] - 7s 883ms/step - loss: 0.2275 - accuracy: 0.9098 - val_loss: 0.2387 - val_accuracy: 0.8710\n",
            "Epoch 10/15\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.2252 - accuracy: 0.8934 - val_loss: 0.1128 - val_accuracy: 0.9677\n",
            "Epoch 11/15\n",
            "8/8 [==============================] - 7s 841ms/step - loss: 0.2097 - accuracy: 0.9016 - val_loss: 0.1769 - val_accuracy: 0.9032\n",
            "Epoch 12/15\n",
            "8/8 [==============================] - 6s 806ms/step - loss: 0.1851 - accuracy: 0.9262 - val_loss: 0.1289 - val_accuracy: 0.9355\n",
            "Epoch 13/15\n",
            "8/8 [==============================] - 10s 1s/step - loss: 0.2632 - accuracy: 0.8689 - val_loss: 0.1223 - val_accuracy: 0.9677\n",
            "Epoch 14/15\n",
            "8/8 [==============================] - 6s 819ms/step - loss: 0.1905 - accuracy: 0.9262 - val_loss: 0.0902 - val_accuracy: 0.9677\n",
            "Epoch 15/15\n",
            "8/8 [==============================] - 8s 1s/step - loss: 0.1441 - accuracy: 0.9508 - val_loss: 0.0776 - val_accuracy: 0.9677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(128, 128))\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(fn)\n",
        "  print(classes)\n",
        "     "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 290
        },
        "id": "JUMtSCGCTMYW",
        "outputId": "46f2481f-2149-420f-8e17-5e8033843212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7260f89e-4ea7-43a9-8214-0a345a2f4f6e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7260f89e-4ea7-43a9-8214-0a345a2f4f6e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dog210508_07_02_21_pic0.jpg to dog210508_07_02_21_pic0.jpg\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-f09f8eb7fec0>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# predicting images\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m   \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'keras.preprocessing.image' has no attribute 'load_img'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from google.colab import files\n",
        "from tensorflow.keras.preprocessing import image  # Import from tf.keras instead of keras\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  # Predicting images\n",
        "  path = fn\n",
        "  img = image.load_img(path, target_size=(128, 128, 3))  # Use tf.keras.preprocessing.image.load_img\n",
        "  x = image.img_to_array(img)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(fn)\n",
        "  print(classes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "WiWGTfTQUDvY",
        "outputId": "8a030074-741f-48fa-c09a-93982ef45c09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-47e821cb-c12e-440f-8f53-ec27659852b2\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-47e821cb-c12e-440f-8f53-ec27659852b2\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving dog210424_05_01_31_pic0.jpg to dog210424_05_01_31_pic0.jpg\n",
            "Saving dog210612_48_02_12_pic0.jpg to dog210612_48_02_12_pic0.jpg\n",
            "Saving dog210617_01_02_05_pic0.jpg to dog210617_01_02_05_pic0.jpg\n",
            "Saving dog210617_02_02_01_pic0.jpg to dog210617_02_02_01_pic0.jpg\n",
            "Saving Dog210621_01_01_43_pic0.jpg to Dog210621_01_01_43_pic0.jpg\n",
            "Saving Dog210622_01_01_39_pic0.jpg to Dog210622_01_01_39_pic0.jpg\n",
            "Saving Dog210627_08_02_24_pic0.jpg to Dog210627_08_02_24_pic0.jpg\n",
            "Saving Dog210704_08_02_01_pic0.jpg to Dog210704_08_02_01_pic0.jpg\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "dog210424_05_01_31_pic0.jpg\n",
            "[[1.]]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "dog210612_48_02_12_pic0.jpg\n",
            "[[1.]]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "dog210617_01_02_05_pic0.jpg\n",
            "[[1.]]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "dog210617_02_02_01_pic0.jpg\n",
            "[[1.]]\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "Dog210621_01_01_43_pic0.jpg\n",
            "[[1.]]\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Dog210622_01_01_39_pic0.jpg\n",
            "[[1.]]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Dog210627_08_02_24_pic0.jpg\n",
            "[[1.]]\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "Dog210704_08_02_01_pic0.jpg\n",
            "[[1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_labels = ['allergic', 'non-allergic']\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "    # Predicting images\n",
        "    path = fn\n",
        "    img = image.load_img(path, target_size=(128, 128))\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    images = np.vstack([x])\n",
        "    classes = model.predict(images, batch_size=10)\n",
        "    predicted_label = class_labels[int(classes[0][0])]\n",
        "    \n",
        "    print(fn)\n",
        "    print(predicted_label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8ngKQZVU0oZ",
        "outputId": "de5a2bce-f8ad-4be9-df82-d2cafe47fa84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 21ms/step\n",
            "dog210424_05_01_31_pic0.jpg\n",
            "non-allergic\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "dog210612_48_02_12_pic0.jpg\n",
            "non-allergic\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "dog210617_01_02_05_pic0.jpg\n",
            "non-allergic\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "dog210617_02_02_01_pic0.jpg\n",
            "non-allergic\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "Dog210621_01_01_43_pic0.jpg\n",
            "non-allergic\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "Dog210622_01_01_39_pic0.jpg\n",
            "non-allergic\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "Dog210627_08_02_24_pic0.jpg\n",
            "non-allergic\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "Dog210704_08_02_01_pic0.jpg\n",
            "non-allergic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load the pre-trained ResNet50 model without the top layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))"
      ],
      "metadata": {
        "id": "w_bQusmoWM1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "\n",
        "# Create a new model by adding your own classifier on top of the pre-trained ResNet model\n",
        "model = models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "AQciyQCZWVg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, epochs=10, validation_data=validation_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 608
        },
        "id": "LuZQ6-HjWcmR",
        "outputId": "3f7964a7-520f-4058-fb36-fa2cac03a243"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "7/7 [==============================] - 70s 7s/step - loss: 2.0308 - accuracy: 0.6937 - val_loss: 2025.8903 - val_accuracy: 0.6429\n",
            "Epoch 2/10\n",
            "7/7 [==============================] - 44s 6s/step - loss: 0.2158 - accuracy: 0.9189 - val_loss: 143049.0156 - val_accuracy: 0.6429\n",
            "Epoch 3/10\n",
            "7/7 [==============================] - 43s 6s/step - loss: 0.2387 - accuracy: 0.9459 - val_loss: 532021.9375 - val_accuracy: 0.6429\n",
            "Epoch 4/10\n",
            "7/7 [==============================] - 47s 7s/step - loss: 0.1038 - accuracy: 0.9640 - val_loss: 515740.7812 - val_accuracy: 0.6429\n",
            "Epoch 5/10\n",
            "7/7 [==============================] - 44s 6s/step - loss: 0.1261 - accuracy: 0.9910 - val_loss: 254101.0938 - val_accuracy: 0.6429\n",
            "Epoch 6/10\n",
            "7/7 [==============================] - 46s 6s/step - loss: 0.2518 - accuracy: 0.9459 - val_loss: 105589.9453 - val_accuracy: 0.6429\n",
            "Epoch 7/10\n",
            "7/7 [==============================] - ETA: 0s - loss: 0.0603 - accuracy: 0.9910"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-f1e6b0f8bc58>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1727\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                         )\n\u001b[0;32m-> 1729\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1730\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Initialize the Pretrained Model\n",
        "feature_extractor = ResNet50(weights='imagenet', \n",
        "                             input_shape=(128, 128, 3),\n",
        "                             include_top=False)\n",
        "\n",
        "# Set this parameter to make sure it's not being trained\n",
        "feature_extractor.trainable = False\n",
        "\n",
        "# Set the input layer\n",
        "input_ = tf.keras.Input(shape=(128, 128, 3))\n",
        "\n",
        "# Set the feature extractor layer\n",
        "x = feature_extractor(input_, training=False)\n",
        "\n",
        "# Set the pooling layer\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Set the final layer with sigmoid activation function\n",
        "output_ = tf.keras.layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the new model object\n",
        "model = tf.keras.Model(input_, output_)\n",
        "\n",
        "# Compile it\n",
        "model.compile(optimizer='adam',\n",
        "             loss='binary_crossentropy',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "# Print The Summary of The Model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edlbXJQLG9g4",
        "outputId": "f604e89e-dbb3-4189-a0aa-b2e3652651c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 0s 0us/step\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 128, 128, 3)]     0         \n",
            "                                                                 \n",
            " resnet50 (Functional)       (None, 4, 4, 2048)        23587712  \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 2048)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 1)                 2049      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,589,761\n",
            "Trainable params: 2,049\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_generator, epochs=15, validation_data=validation_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6L_WcSk1HOag",
        "outputId": "17bf53e6-2c0c-4dda-8b87-40728601805d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "6/6 [==============================] - 16s 2s/step - loss: 0.5639 - accuracy: 0.7622 - val_loss: 0.5571 - val_accuracy: 0.7561\n",
            "Epoch 2/15\n",
            "6/6 [==============================] - 9s 1s/step - loss: 0.5496 - accuracy: 0.7622 - val_loss: 0.5528 - val_accuracy: 0.7561\n",
            "Epoch 3/15\n",
            "6/6 [==============================] - 8s 1s/step - loss: 0.5453 - accuracy: 0.7622 - val_loss: 0.5533 - val_accuracy: 0.7561\n",
            "Epoch 4/15\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.5445 - accuracy: 0.7622 - val_loss: 0.5525 - val_accuracy: 0.7561\n",
            "Epoch 5/15\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.5416 - accuracy: 0.7622 - val_loss: 0.5491 - val_accuracy: 0.7561\n",
            "Epoch 6/15\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.5379 - accuracy: 0.7622 - val_loss: 0.5469 - val_accuracy: 0.7561\n",
            "Epoch 7/15\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.5353 - accuracy: 0.7622 - val_loss: 0.5446 - val_accuracy: 0.7561\n",
            "Epoch 8/15\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.5324 - accuracy: 0.7622 - val_loss: 0.5429 - val_accuracy: 0.7561\n",
            "Epoch 9/15\n",
            "6/6 [==============================] - 8s 1s/step - loss: 0.5303 - accuracy: 0.7622 - val_loss: 0.5422 - val_accuracy: 0.7561\n",
            "Epoch 10/15\n",
            "6/6 [==============================] - 10s 1s/step - loss: 0.5286 - accuracy: 0.7622 - val_loss: 0.5422 - val_accuracy: 0.7561\n",
            "Epoch 11/15\n",
            "6/6 [==============================] - 9s 2s/step - loss: 0.5280 - accuracy: 0.7622 - val_loss: 0.5418 - val_accuracy: 0.7561\n",
            "Epoch 12/15\n",
            "6/6 [==============================] - 8s 1s/step - loss: 0.5276 - accuracy: 0.7622 - val_loss: 0.5394 - val_accuracy: 0.7561\n",
            "Epoch 13/15\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.5241 - accuracy: 0.7622 - val_loss: 0.5391 - val_accuracy: 0.7561\n",
            "Epoch 14/15\n",
            "6/6 [==============================] - 10s 2s/step - loss: 0.5227 - accuracy: 0.7622 - val_loss: 0.5361 - val_accuracy: 0.7561\n",
            "Epoch 15/15\n",
            "6/6 [==============================] - 8s 1s/step - loss: 0.5189 - accuracy: 0.7622 - val_loss: 0.5325 - val_accuracy: 0.7561\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f6ea67842b0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    }
  ]
}