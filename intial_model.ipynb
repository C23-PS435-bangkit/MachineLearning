{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C23-PS435-bangkit/MachineLearning/blob/normal-dataset-only/intial_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libs"
      ],
      "metadata": {
        "id": "me6oML2PklmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "uJzZqF4Dkt3X"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TQN4Fe4mAyWj",
        "outputId": "eca8795f-9223-4ec9-ce72-25d15e79d300",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Split Datasets"
      ],
      "metadata": {
        "id": "8XGjuWNYkqh-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "check the datasets"
      ],
      "metadata": {
        "id": "o1Jee5iRKc_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/dataset anjing/normal dataset'\n",
        "\n",
        "bd_dir = os.path.join(base_dir, 'bacterial_dermatosis')\n",
        "fi_dir = os.path.join(base_dir, 'fungal_infection')\n",
        "ha_dir = os.path.join(base_dir, 'hypersensitivity_allergic_dermatosis')\n",
        "he_dir = os.path.join(base_dir, 'healthy')\n",
        "\n",
        "print('total training bacterial_dermatosis images:', len(os.listdir(bd_dir)))\n",
        "print('total training fungal_infection images:', len(os.listdir(fi_dir)))\n",
        "print('total training hypersensitivity_allergic_dermatosis images:', len(os.listdir(ha_dir)))\n",
        "print('total training healthy images:', len(os.listdir(he_dir)))\n",
        "\n",
        "bd_files = os.listdir(bd_dir)\n",
        "print(bd_files[:10])\n",
        "\n",
        "fi_files = os.listdir(fi_dir)\n",
        "print(fi_files[:10])\n",
        "\n",
        "ha_files = os.listdir(ha_dir)\n",
        "print(ha_files[:10])\n",
        "\n",
        "he_files = os.listdir(he_dir)\n",
        "print(he_files[:10])"
      ],
      "metadata": {
        "id": "3s9Oe0YOC9zr",
        "outputId": "4b29cfe8-8c1c-43b5-a94c-b580e26bbcc0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training bacterial_dermatosis images: 12\n",
            "total training fungal_infection images: 11\n",
            "total training hypersensitivity_allergic_dermatosis images: 13\n",
            "total training healthy images: 26\n",
            "['dog210612_03_01_13_pic0.jpg', 'dog210617_00_02_02_pic0.jpg', 'dog210615_01_02_06_pic0.jpg', 'Dog210619_01_01_44_pic0.jpg', 'Dog210624_08_02_35_pic0.jpg', 'Dog210628_04_01_17_pic0.jpg', 'Dog210628_11_01_16_pic0.jpg', 'Dog210629_09_02_13_pic0.jpg', 'Dog210630_04_02_11_pic0.jpg', 'Dog210702_09_01_09_pic0.jpg']\n",
            "['dog210422_04_02_33_pic0.jpg', 'dog210424_09_02_32_pic0.jpg', 'dog210430_05_01_29_pic0.jpg', 'dog210430_08_01_30_pic0.jpg', 'dog210504_49_01_28_pic0.jpg', 'dog210610_04_02_14_pic0.jpg', 'dog210610_48_02_15_pic0.jpg', 'Dog210622_03_02_41_pic0.jpg', 'Dog210623_08_02_37_pic0.jpg', 'Dog210706_01_02_06_pic0.jpg']\n",
            "['dog210424_05_01_31_pic0.jpg', 'dog210612_48_02_12_pic0.jpg', 'dog210617_02_02_01_pic0.jpg', 'dog210617_01_02_05_pic0.jpg', 'Dog210621_01_01_43_pic0.jpg', 'Dog210622_01_01_39_pic0.jpg', 'Dog210623_05_01_38_pic0.jpg', 'Dog210624_01_02_36_pic0.jpg', 'Dog210624_12_02_32_pic0.jpg', 'Dog210625_05_02_30_pic0.jpg']\n",
            "['dog210505_01_02_26_pic0.jpg', 'dog210505_01_02_25_pic0.jpg', 'dog210505_01_02_27_pic0.jpg', 'dog210508_06_02_19_pic0.jpg', 'dog210508_06_02_18_pic0.jpg', 'dog210508_06_02_20_pic0.jpg', 'dog210508_07_02_21_pic0.jpg', 'dog210508_07_02_22_pic0.jpg', 'dog210508_07_02_23_pic0.jpg', 'dog210511_05_02_17_pic0.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## bd"
      ],
      "metadata": {
        "id": "5vab9j8IlZ5q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### manage new directory structure"
      ],
      "metadata": {
        "id": "eUTsna4zlgV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Desired structure:\n",
        "\n",
        "```\n",
        "bd modelling\n",
        "  |--- bd_dir (images of bacterial_dermatosis)\n",
        "  |   |--- img1.png\n",
        "  |   |--- img2.png\n",
        "  |   |--- ...\n",
        "  |            \n",
        "  |--- non_bd_dir (images if fi, ha, he)\n",
        "      |--- img1.png\n",
        "      |--- img2.png\n",
        "      |--- ...\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "VKWUlepEyJIC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to make new dir with new structured image dataset to satisfy the need of bd_modelling"
      ],
      "metadata": {
        "id": "fDrPMLgSluyV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "def create_modelling_dataset(base_dir, bd_dir, fi_dir, ha_dir, he_dir):\n",
        "    # Create the 'bd_modelling' directory inside 'base_dir'\n",
        "    bd_modelling_dir = os.path.join(base_dir, 'bd_modelling')\n",
        "    os.makedirs(bd_modelling_dir, exist_ok=True)\n",
        "\n",
        "    # Create the 'bd_dir' directory inside 'bd_modelling'\n",
        "    bd_dir_modelling = os.path.join(bd_modelling_dir, 'bd_dir')\n",
        "    os.makedirs(bd_dir_modelling, exist_ok=True)\n",
        "\n",
        "    # Copy all images from 'bd_dir' to 'bd_dir_modelling'\n",
        "    bd_files = os.listdir(bd_dir)\n",
        "    for file in bd_files:\n",
        "        src_path = os.path.join(bd_dir, file)\n",
        "        dst_path = os.path.join(bd_dir_modelling, file)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "    # Create the 'non_bd_dir' directory inside 'bd_modelling'\n",
        "    non_bd_dir_modelling = os.path.join(bd_modelling_dir, 'non_bd_dir')\n",
        "    os.makedirs(non_bd_dir_modelling, exist_ok=True)\n",
        "\n",
        "    # Copy all images from the rest of the directories to 'non_bd_dir_modelling'\n",
        "    for dir_path in [fi_dir, ha_dir, he_dir]:\n",
        "        files = os.listdir(dir_path)\n",
        "        for file in files:\n",
        "            src_path = os.path.join(dir_path, file)\n",
        "            dst_path = os.path.join(non_bd_dir_modelling, file)\n",
        "            shutil.copy(src_path, dst_path)\n",
        "\n",
        "    print(\"Modelling dataset created successfully.\")"
      ],
      "metadata": {
        "id": "UxoU2MSZlcCt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_modelling_dataset(base_dir, bd_dir, fi_dir, ha_dir, he_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lpIAUl-l_Ic",
        "outputId": "f766c626-15fc-4ec3-ae15-fc4c4b2062e7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelling dataset created successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Crop & Augment Images"
      ],
      "metadata": {
        "id": "UbVwtCIppt-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_progress(progress):\n",
        "    bar_length = 20  # Length of the progress bar\n",
        "    filled_length = int(bar_length * progress)\n",
        "    bar = 'â–ˆ' * filled_length + '-' * (bar_length - filled_length)\n",
        "    percentage = int(progress * 100)\n",
        "    print(f'\\rProgress: |{bar}| {percentage}% ', end='', flush=True)"
      ],
      "metadata": {
        "id": "S9xnjWlgjQ4U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to crop image\n",
        "# note: the amount of croped pixels were approxed by me (nopal) \n",
        "#       by measure the excess pixels\n",
        "def crop_image(image):\n",
        "    crop_width = 1920 - 170 - 150  # Calculate the resulting width after trimming\n",
        "    crop_height = 1080  # Height remains the same\n",
        "    crop_location = (170, 0)  # Starting position of the crop\n",
        "\n",
        "    # Crop the image using array indexing\n",
        "    cropped_image = image.crop((crop_location[0], crop_location[1],\n",
        "                                crop_location[0] + crop_width, crop_location[1] + crop_height))\n",
        "\n",
        "    return cropped_image"
      ],
      "metadata": {
        "id": "kJ3RPYBv0Vav"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    # width_shift_range=0.1,\n",
        "    # height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    # zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "4Rq5t8xP0a5h"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import shutil\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "def augment_and_save_images(src_dir, target_dir, datagen, amount):\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    for file_name in os.listdir(src_dir):\n",
        "        file_path = os.path.join(src_dir, file_name)\n",
        "\n",
        "        # Load the image using PIL\n",
        "        image = Image.open(file_path)\n",
        "\n",
        "        # Crop the image using the crop_image() function\n",
        "        cropped_image = crop_image(image)\n",
        "\n",
        "        # Expand dimensions to match the expected input shape of datagen.flow\n",
        "        image_array = np.expand_dims(cropped_image, axis=0)\n",
        "\n",
        "        # Generate 20 augmented images using the data generator\n",
        "        augmented_images = datagen.flow(image_array, batch_size=1, save_to_dir=target_dir, save_prefix='aug_', save_format='jpeg')\n",
        "\n",
        "        print(f\"\\nAugmenting {file_name}\")\n",
        "\n",
        "        # Iterate over the augmented images and save them to the target directory\n",
        "        for i, augmented_image in enumerate(augmented_images):\n",
        "            if i >= amount:\n",
        "                break\n",
        "            update_progress((i+1)/amount)\n",
        "\n",
        "    print(\"\\nAugmentation completed successfully.\")"
      ],
      "metadata": {
        "id": "h8iFoIjS0SLd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir = '/content/drive/MyDrive/dataset anjing/bd_modelling/bd_dir'\n",
        "target_dir = '/content/drive/MyDrive/dataset anjing/bd_modelling_aug/bd_dir'\n",
        "augment_and_save_images(src_dir, target_dir, datagen, 10)"
      ],
      "metadata": {
        "id": "oLqahiS10e9I",
        "outputId": "e31f11ac-86d4-41f4-c00b-4f28ddba1fd0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Augmenting dog210612_03_01_13_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210617_00_02_02_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210615_01_02_06_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210619_01_01_44_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210624_08_02_35_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210628_04_01_17_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210628_11_01_16_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210629_09_02_13_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210630_04_02_11_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210702_09_01_09_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210706_48_02_05_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210707_01_01_04_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmentation completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_dir = '/content/drive/MyDrive/dataset anjing/bd_modelling/non_bd_dir'\n",
        "target_dir = '/content/drive/MyDrive/dataset anjing/bd_modelling_aug/non_bd_dir'\n",
        "augment_and_save_images(src_dir, target_dir, datagen, 5)"
      ],
      "metadata": {
        "id": "lezinildnde0",
        "outputId": "515ac310-9906-4639-9960-8231b6cd3171",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Augmenting dog210422_04_02_33_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210424_09_02_32_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210430_05_01_29_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210430_08_01_30_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210504_49_01_28_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210610_04_02_14_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210610_48_02_15_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210622_03_02_41_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210623_08_02_37_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210706_01_02_06_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210708_01_02_01_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210424_05_01_31_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210612_48_02_12_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210617_02_02_01_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210617_01_02_05_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210621_01_01_43_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210622_01_01_39_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210623_05_01_38_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210624_01_02_36_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210624_12_02_32_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210625_05_02_30_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210627_08_01_22_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210627_08_02_24_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210704_08_02_01_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210505_01_02_26_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210505_01_02_25_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210505_01_02_27_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210508_06_02_19_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210508_06_02_18_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210508_06_02_20_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210508_07_02_21_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210508_07_02_22_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210508_07_02_23_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210511_05_02_17_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210617_01_02_03_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting dog210617_01_02_04_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210621_13_02_42_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210622_01_01_40_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210625_01_02_26_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210625_01_02_27_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210625_01_02_28_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210625_01_02_29_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210627_01_01_21_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210628_01_02_18_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210629_08_02_15_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210702_08_02_07_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210703_08_01_06_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210705_09_02_07_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210708_02_01_03_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmenting Dog210708_48_01_02_pic0.jpg\n",
            "Progress: |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100% \n",
            "Augmentation completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data finalization"
      ],
      "metadata": {
        "id": "Okh6uqDqwd9U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "# Function to copy all files from src_dir to target_dir\n",
        "\n",
        "def copy_files(src_dir, target_dir):\n",
        "    # Get a list of all files in the source directory\n",
        "    files = os.listdir(src_dir)\n",
        "\n",
        "    # Copy each file to the destination directory\n",
        "    for file in files:\n",
        "        src_path = os.path.join(src_dir, file)\n",
        "        dest_path = os.path.join(target_dir, file)\n",
        "        shutil.copy(src_path, dest_path)"
      ],
      "metadata": {
        "id": "5aDjoTDFyYv0"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine all bd images (augmented & non augmented)\n",
        "copy_files('/content/drive/MyDrive/dataset anjing/bd_modelling/bd_dir', '/content/drive/MyDrive/dataset anjing/bd_modelling_final/bd_dir')\n",
        "copy_files('/content/drive/MyDrive/dataset anjing/bd_modelling_aug/bd_dir', '/content/drive/MyDrive/dataset anjing/bd_modelling_final/bd_dir')"
      ],
      "metadata": {
        "id": "WkVauXoXyio6"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combine all non_bd images (augmented & non augmented)\n",
        "copy_files('/content/drive/MyDrive/dataset anjing/bd_modelling/non_bd_dir', '/content/drive/MyDrive/dataset anjing/bd_modelling_final/non_bd_dir')\n",
        "copy_files('/content/drive/MyDrive/dataset anjing/bd_modelling_aug/non_bd_dir', '/content/drive/MyDrive/dataset anjing/bd_modelling_final/non_bd_dir')"
      ],
      "metadata": {
        "id": "vJulVZZizoIj"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir('/content/drive/MyDrive/dataset anjing/bd_modelling_final/bd_dir')))"
      ],
      "metadata": {
        "id": "DxH6geLR0oJc",
        "outputId": "6061c7ba-3a06-46ec-dbc6-186f094f4038",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir('/content/drive/MyDrive/dataset anjing/bd_modelling_final/non_bd_dir')))"
      ],
      "metadata": {
        "id": "q9Xro46S0e7C",
        "outputId": "5fc4dc02-ce0d-4d9f-ae7b-e7176bd4ab2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "D9xHSYZd1rNN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Undersample"
      ],
      "metadata": {
        "id": "WZJb69skCwb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "def perform_random_undersampling(base_dir, majority_dir, minority_dir, target_dir, desired_ratio):\n",
        "    os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "    majority_files = os.listdir(os.path.join(base_dir, majority_dir))\n",
        "    minority_files = os.listdir(os.path.join(base_dir, minority_dir))\n",
        "\n",
        "    majority_count = len(majority_files)\n",
        "    minority_count = len(minority_files)\n",
        "\n",
        "    # Calculate the number of samples to randomly select from the majority class\n",
        "    max_majority_samples = int(minority_count * desired_ratio)\n",
        "\n",
        "    # Randomly select samples from the majority class\n",
        "    selected_majority_samples = random.sample(majority_files, max_majority_samples)\n",
        "\n",
        "    # Copy the selected majority samples to the target directory\n",
        "    for file_name in selected_majority_samples:\n",
        "        src_path = os.path.join(base_dir, majority_dir, file_name)\n",
        "        dst_path = os.path.join(base_dir, target_dir, majority_dir, file_name)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "    # Copy all samples from the minority class to the target directory\n",
        "    for file_name in minority_files:\n",
        "        src_path = os.path.join(base_dir, minority_dir, file_name)\n",
        "        dst_path = os.path.join(base_dir, target_dir, minority_dir, file_name)\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "    print(\"Random undersampling completed successfully.\")\n"
      ],
      "metadata": {
        "id": "eq3Cw1a_2GZb"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Example usage\n",
        "base_dir = '/content/drive/MyDrive/dataset anjing/bd_modelling_final/'\n",
        "majority_dir = 'non_bd_dir'\n",
        "minority_dir = 'bd_dir'\n",
        "target_dir = 'undersampled'\n",
        "desired_ratio = 1.0\n",
        "\n",
        "perform_random_undersampling(base_dir, majority_dir, minority_dir, target_dir, desired_ratio)\n"
      ],
      "metadata": {
        "id": "5wcKINcOC7g0",
        "outputId": "33ef366b-7d94-4ab8-c230-5e0fd61aca23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random undersampling completed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir('/content/drive/MyDrive/dataset anjing/bd_modelling_final/undersampled/bd_dir')))"
      ],
      "metadata": {
        "id": "BR4A9pSjETaX",
        "outputId": "7e8bf6a1-9844-497d-b786-fda3b494c5ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(os.listdir('/content/drive/MyDrive/dataset anjing/bd_modelling_final/undersampled/non_bd_dir')))"
      ],
      "metadata": {
        "id": "dQ0w6NXeEWx-",
        "outputId": "4e3c481e-b31d-4419-ef08-fb5cf758c530",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split data into training set & validation set"
      ],
      "metadata": {
        "id": "Dk05TVLK1tm6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/drive/MyDrive/dataset anjing/bd_final'\n",
        "\n",
        "# Create train directory\n",
        "train_cats_dir = os.path.join(root_dir, 'training/bd')\n",
        "os.makedirs(train_cats_dir)\n",
        "train_dogs_dir = os.path.join(root_dir, 'training/non_bd')\n",
        "os.makedirs(train_dogs_dir)\n",
        "# Create validation directory\n",
        "val_cats_dir = os.path.join(root_dir, 'validation/bd')\n",
        "os.makedirs(val_cats_dir)\n",
        "val_dogs_dir = os.path.join(root_dir, 'validation/non_bd')\n",
        "os.makedirs(val_dogs_dir)"
      ],
      "metadata": {
        "id": "jQMtb15GEerq"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for rootdir, dirs, files in os.walk(root_dir):\n",
        "    for subdir in dirs:\n",
        "        print(os.path.join(rootdir, subdir))"
      ],
      "metadata": {
        "id": "Bi_arp1TE4Zi",
        "outputId": "8a176a21-e1b2-4fa5-84f4-10ebf44887ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/dataset anjing/bd_final/training\n",
            "/content/drive/MyDrive/dataset anjing/bd_final/validation\n",
            "/content/drive/MyDrive/dataset anjing/bd_final/training/bd\n",
            "/content/drive/MyDrive/dataset anjing/bd_final/training/non_bd\n",
            "/content/drive/MyDrive/dataset anjing/bd_final/validation/bd\n",
            "/content/drive/MyDrive/dataset anjing/bd_final/validation/non_bd\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
        "  # get list of files\n",
        "  files = os.listdir(SOURCE_DIR)\n",
        "\n",
        "  # filter out files with zero length\n",
        "  verified_files = []\n",
        "  for file in files :\n",
        "    if os.path.getsize(os.path.join(SOURCE_DIR, file)) == 0 :\n",
        "      print(f\"{file} is zero length, so ignoring.\")\n",
        "    else :\n",
        "      verified_files.append(file)\n",
        "      \n",
        "\n",
        "  # shuffle the files\n",
        "  random.shuffle(verified_files)\n",
        "\n",
        "  # calculate the split index\n",
        "  split_idx = int(SPLIT_SIZE * len(verified_files))\n",
        "\n",
        "  # split the files\n",
        "  train_files = verified_files[:split_idx]\n",
        "  val_files = verified_files[split_idx:]\n",
        "\n",
        "  # copy train files\n",
        "  for file in train_files:\n",
        "      src_path = os.path.join(SOURCE_DIR, file)\n",
        "      dst_path = os.path.join(TRAINING_DIR, file)\n",
        "      shutil.copy(src_path, dst_path)\n",
        "\n",
        "  # copy validation files\n",
        "  for file in val_files:\n",
        "      src_path = os.path.join(SOURCE_DIR, file)\n",
        "      dst_path = os.path.join(VALIDATION_DIR, file)\n",
        "      shutil.copy(src_path, dst_path)"
      ],
      "metadata": {
        "id": "D1qY6jUeGEov"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths\n",
        "bd_src_dir = '/content/drive/MyDrive/dataset anjing/bd_modelling_final/undersampled/bd_dir'\n",
        "non_bd_src_dir = '/content/drive/MyDrive/dataset anjing/bd_modelling_final/undersampled/non_bd_dir'\n",
        "\n",
        "train_dir = \"/content/drive/MyDrive/dataset anjing/bd_final/training\"\n",
        "val_dir = \"/content/drive/MyDrive/dataset anjing/bd_final/validation\"\n",
        "\n",
        "bd_train_dir = os.path.join(train_dir, \"bd/\")\n",
        "bd_val_dir = os.path.join(val_dir, \"bd/\")\n",
        "\n",
        "non_bd_train_dir = os.path.join(train_dir, \"non_bd/\")\n",
        "non_bd_val_dir = os.path.join(val_dir, \"non_bd/\")\n",
        "\n",
        "# Empty directories in case you run this cell multiple times\n",
        "if len(os.listdir(bd_train_dir)) > 0:\n",
        "  for file in os.scandir(bd_train_dir):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(non_bd_train_dir)) > 0:\n",
        "  for file in os.scandir(non_bd_train_dir):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(bd_val_dir)) > 0:\n",
        "  for file in os.scandir(bd_val_dir):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(non_bd_val_dir)) > 0:\n",
        "  for file in os.scandir(non_bd_val_dir):\n",
        "    os.remove(file.path)\n",
        "\n",
        "# Define proportion of images used for training\n",
        "split_size = .8\n",
        "\n",
        "# Run the function\n",
        "# NOTE: Messages about zero length images should be printed out\n",
        "split_data(bd_src_dir, bd_train_dir, bd_val_dir, split_size)\n",
        "split_data(non_bd_src_dir, non_bd_train_dir, non_bd_val_dir, split_size)\n",
        "\n",
        "# Your function should perform copies rather than moving images so original directories should contain unchanged images\n",
        "print(f\"\\n\\nOriginal bd directory has {len(os.listdir(bd_src_dir))} images\")\n",
        "print(f\"Original non_bd directory has {len(os.listdir(non_bd_src_dir))} images\\n\")\n",
        "\n",
        "# Training and validation splits. Check that the number of images matches the expected output.\n",
        "print(f\"There are {len(os.listdir(bd_train_dir))} images of bd for training\")\n",
        "print(f\"There are {len(os.listdir(non_bd_train_dir))} images of non_bd for training\")\n",
        "print(f\"There are {len(os.listdir(bd_val_dir))} images of bd for validation\")\n",
        "print(f\"There are {len(os.listdir(non_bd_val_dir))} images of non_bd for validation\")"
      ],
      "metadata": {
        "id": "D0cQx-pBGP3K",
        "outputId": "46034dbb-2054-4bf8-8dd6-55ed2ee3de85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Original bd directory has 143 images\n",
            "Original non_bd directory has 143 images\n",
            "\n",
            "There are 114 images of bd for training\n",
            "There are 114 images of non_bd for training\n",
            "There are 29 images of bd for validation\n",
            "There are 29 images of non_bd for validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transfer Learning on RestNet"
      ],
      "metadata": {
        "id": "yAljvfRJJGxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load the pre-trained ResNet-50 model without the top classification layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Add your own classification layers on top of the pre-trained base model\n",
        "x = base_model.output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "# Create the final model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7bMPUxDxJJrD"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "CAOjhBU3KQhi",
        "outputId": "89642926-36a7-4824-bd8a-86259bdfb839",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None, 150, 150, 3  0           []                               \n",
            "                                )]                                                                \n",
            "                                                                                                  \n",
            " conv1_pad (ZeroPadding2D)      (None, 156, 156, 3)  0           ['input_3[0][0]']                \n",
            "                                                                                                  \n",
            " conv1_conv (Conv2D)            (None, 75, 75, 64)   9472        ['conv1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv1_bn (BatchNormalization)  (None, 75, 75, 64)   256         ['conv1_conv[0][0]']             \n",
            "                                                                                                  \n",
            " conv1_relu (Activation)        (None, 75, 75, 64)   0           ['conv1_bn[0][0]']               \n",
            "                                                                                                  \n",
            " pool1_pad (ZeroPadding2D)      (None, 77, 77, 64)   0           ['conv1_relu[0][0]']             \n",
            "                                                                                                  \n",
            " pool1_pool (MaxPooling2D)      (None, 38, 38, 64)   0           ['pool1_pad[0][0]']              \n",
            "                                                                                                  \n",
            " conv2_block1_1_conv (Conv2D)   (None, 38, 38, 64)   4160        ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_1_bn (BatchNormal  (None, 38, 38, 64)  256         ['conv2_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_1_relu (Activatio  (None, 38, 38, 64)  0           ['conv2_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_2_conv (Conv2D)   (None, 38, 38, 64)   36928       ['conv2_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_2_bn (BatchNormal  (None, 38, 38, 64)  256         ['conv2_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_2_relu (Activatio  (None, 38, 38, 64)  0           ['conv2_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block1_0_conv (Conv2D)   (None, 38, 38, 256)  16640       ['pool1_pool[0][0]']             \n",
            "                                                                                                  \n",
            " conv2_block1_3_conv (Conv2D)   (None, 38, 38, 256)  16640       ['conv2_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block1_0_bn (BatchNormal  (None, 38, 38, 256)  1024       ['conv2_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_3_bn (BatchNormal  (None, 38, 38, 256)  1024       ['conv2_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block1_add (Add)         (None, 38, 38, 256)  0           ['conv2_block1_0_bn[0][0]',      \n",
            "                                                                  'conv2_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block1_out (Activation)  (None, 38, 38, 256)  0           ['conv2_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_conv (Conv2D)   (None, 38, 38, 64)   16448       ['conv2_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block2_1_bn (BatchNormal  (None, 38, 38, 64)  256         ['conv2_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_1_relu (Activatio  (None, 38, 38, 64)  0           ['conv2_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_2_conv (Conv2D)   (None, 38, 38, 64)   36928       ['conv2_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_2_bn (BatchNormal  (None, 38, 38, 64)  256         ['conv2_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_2_relu (Activatio  (None, 38, 38, 64)  0           ['conv2_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block2_3_conv (Conv2D)   (None, 38, 38, 256)  16640       ['conv2_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block2_3_bn (BatchNormal  (None, 38, 38, 256)  1024       ['conv2_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block2_add (Add)         (None, 38, 38, 256)  0           ['conv2_block1_out[0][0]',       \n",
            "                                                                  'conv2_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block2_out (Activation)  (None, 38, 38, 256)  0           ['conv2_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_conv (Conv2D)   (None, 38, 38, 64)   16448       ['conv2_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv2_block3_1_bn (BatchNormal  (None, 38, 38, 64)  256         ['conv2_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_1_relu (Activatio  (None, 38, 38, 64)  0           ['conv2_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_2_conv (Conv2D)   (None, 38, 38, 64)   36928       ['conv2_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_2_bn (BatchNormal  (None, 38, 38, 64)  256         ['conv2_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_2_relu (Activatio  (None, 38, 38, 64)  0           ['conv2_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv2_block3_3_conv (Conv2D)   (None, 38, 38, 256)  16640       ['conv2_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv2_block3_3_bn (BatchNormal  (None, 38, 38, 256)  1024       ['conv2_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv2_block3_add (Add)         (None, 38, 38, 256)  0           ['conv2_block2_out[0][0]',       \n",
            "                                                                  'conv2_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv2_block3_out (Activation)  (None, 38, 38, 256)  0           ['conv2_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_conv (Conv2D)   (None, 19, 19, 128)  32896       ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_1_bn (BatchNormal  (None, 19, 19, 128)  512        ['conv3_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_1_relu (Activatio  (None, 19, 19, 128)  0          ['conv3_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_2_conv (Conv2D)   (None, 19, 19, 128)  147584      ['conv3_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_2_bn (BatchNormal  (None, 19, 19, 128)  512        ['conv3_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_2_relu (Activatio  (None, 19, 19, 128)  0          ['conv3_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block1_0_conv (Conv2D)   (None, 19, 19, 512)  131584      ['conv2_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block1_3_conv (Conv2D)   (None, 19, 19, 512)  66048       ['conv3_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block1_0_bn (BatchNormal  (None, 19, 19, 512)  2048       ['conv3_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_3_bn (BatchNormal  (None, 19, 19, 512)  2048       ['conv3_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block1_add (Add)         (None, 19, 19, 512)  0           ['conv3_block1_0_bn[0][0]',      \n",
            "                                                                  'conv3_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block1_out (Activation)  (None, 19, 19, 512)  0           ['conv3_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_conv (Conv2D)   (None, 19, 19, 128)  65664       ['conv3_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block2_1_bn (BatchNormal  (None, 19, 19, 128)  512        ['conv3_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_1_relu (Activatio  (None, 19, 19, 128)  0          ['conv3_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_2_conv (Conv2D)   (None, 19, 19, 128)  147584      ['conv3_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_2_bn (BatchNormal  (None, 19, 19, 128)  512        ['conv3_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_2_relu (Activatio  (None, 19, 19, 128)  0          ['conv3_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block2_3_conv (Conv2D)   (None, 19, 19, 512)  66048       ['conv3_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block2_3_bn (BatchNormal  (None, 19, 19, 512)  2048       ['conv3_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block2_add (Add)         (None, 19, 19, 512)  0           ['conv3_block1_out[0][0]',       \n",
            "                                                                  'conv3_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block2_out (Activation)  (None, 19, 19, 512)  0           ['conv3_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_conv (Conv2D)   (None, 19, 19, 128)  65664       ['conv3_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block3_1_bn (BatchNormal  (None, 19, 19, 128)  512        ['conv3_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_1_relu (Activatio  (None, 19, 19, 128)  0          ['conv3_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_2_conv (Conv2D)   (None, 19, 19, 128)  147584      ['conv3_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_2_bn (BatchNormal  (None, 19, 19, 128)  512        ['conv3_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_2_relu (Activatio  (None, 19, 19, 128)  0          ['conv3_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block3_3_conv (Conv2D)   (None, 19, 19, 512)  66048       ['conv3_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block3_3_bn (BatchNormal  (None, 19, 19, 512)  2048       ['conv3_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block3_add (Add)         (None, 19, 19, 512)  0           ['conv3_block2_out[0][0]',       \n",
            "                                                                  'conv3_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block3_out (Activation)  (None, 19, 19, 512)  0           ['conv3_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_conv (Conv2D)   (None, 19, 19, 128)  65664       ['conv3_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv3_block4_1_bn (BatchNormal  (None, 19, 19, 128)  512        ['conv3_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_1_relu (Activatio  (None, 19, 19, 128)  0          ['conv3_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_2_conv (Conv2D)   (None, 19, 19, 128)  147584      ['conv3_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_2_bn (BatchNormal  (None, 19, 19, 128)  512        ['conv3_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_2_relu (Activatio  (None, 19, 19, 128)  0          ['conv3_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv3_block4_3_conv (Conv2D)   (None, 19, 19, 512)  66048       ['conv3_block4_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv3_block4_3_bn (BatchNormal  (None, 19, 19, 512)  2048       ['conv3_block4_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv3_block4_add (Add)         (None, 19, 19, 512)  0           ['conv3_block3_out[0][0]',       \n",
            "                                                                  'conv3_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv3_block4_out (Activation)  (None, 19, 19, 512)  0           ['conv3_block4_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_conv (Conv2D)   (None, 10, 10, 256)  131328      ['conv3_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block1_1_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_1_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_2_conv (Conv2D)   (None, 10, 10, 256)  590080      ['conv4_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block1_2_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block1_2_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block1_0_conv (Conv2D)   (None, 10, 10, 1024  525312      ['conv3_block4_out[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_conv (Conv2D)   (None, 10, 10, 1024  263168      ['conv4_block1_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_0_bn (BatchNormal  (None, 10, 10, 1024  4096       ['conv4_block1_0_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_3_bn (BatchNormal  (None, 10, 10, 1024  4096       ['conv4_block1_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block1_add (Add)         (None, 10, 10, 1024  0           ['conv4_block1_0_bn[0][0]',      \n",
            "                                )                                 'conv4_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block1_out (Activation)  (None, 10, 10, 1024  0           ['conv4_block1_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_1_conv (Conv2D)   (None, 10, 10, 256)  262400      ['conv4_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block2_1_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_1_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_2_conv (Conv2D)   (None, 10, 10, 256)  590080      ['conv4_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block2_2_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block2_2_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block2_3_conv (Conv2D)   (None, 10, 10, 1024  263168      ['conv4_block2_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_3_bn (BatchNormal  (None, 10, 10, 1024  4096       ['conv4_block2_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block2_add (Add)         (None, 10, 10, 1024  0           ['conv4_block1_out[0][0]',       \n",
            "                                )                                 'conv4_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block2_out (Activation)  (None, 10, 10, 1024  0           ['conv4_block2_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_1_conv (Conv2D)   (None, 10, 10, 256)  262400      ['conv4_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block3_1_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_1_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_2_conv (Conv2D)   (None, 10, 10, 256)  590080      ['conv4_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block3_2_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block3_2_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block3_3_conv (Conv2D)   (None, 10, 10, 1024  263168      ['conv4_block3_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_3_bn (BatchNormal  (None, 10, 10, 1024  4096       ['conv4_block3_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block3_add (Add)         (None, 10, 10, 1024  0           ['conv4_block2_out[0][0]',       \n",
            "                                )                                 'conv4_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block3_out (Activation)  (None, 10, 10, 1024  0           ['conv4_block3_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_1_conv (Conv2D)   (None, 10, 10, 256)  262400      ['conv4_block3_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block4_1_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block4_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_1_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block4_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_2_conv (Conv2D)   (None, 10, 10, 256)  590080      ['conv4_block4_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block4_2_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block4_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block4_2_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block4_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block4_3_conv (Conv2D)   (None, 10, 10, 1024  263168      ['conv4_block4_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_3_bn (BatchNormal  (None, 10, 10, 1024  4096       ['conv4_block4_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block4_add (Add)         (None, 10, 10, 1024  0           ['conv4_block3_out[0][0]',       \n",
            "                                )                                 'conv4_block4_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block4_out (Activation)  (None, 10, 10, 1024  0           ['conv4_block4_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_1_conv (Conv2D)   (None, 10, 10, 256)  262400      ['conv4_block4_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block5_1_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block5_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_1_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block5_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_2_conv (Conv2D)   (None, 10, 10, 256)  590080      ['conv4_block5_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block5_2_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block5_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block5_2_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block5_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block5_3_conv (Conv2D)   (None, 10, 10, 1024  263168      ['conv4_block5_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_3_bn (BatchNormal  (None, 10, 10, 1024  4096       ['conv4_block5_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block5_add (Add)         (None, 10, 10, 1024  0           ['conv4_block4_out[0][0]',       \n",
            "                                )                                 'conv4_block5_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block5_out (Activation)  (None, 10, 10, 1024  0           ['conv4_block5_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_1_conv (Conv2D)   (None, 10, 10, 256)  262400      ['conv4_block5_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv4_block6_1_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block6_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_1_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block6_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_2_conv (Conv2D)   (None, 10, 10, 256)  590080      ['conv4_block6_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv4_block6_2_bn (BatchNormal  (None, 10, 10, 256)  1024       ['conv4_block6_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv4_block6_2_relu (Activatio  (None, 10, 10, 256)  0          ['conv4_block6_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv4_block6_3_conv (Conv2D)   (None, 10, 10, 1024  263168      ['conv4_block6_2_relu[0][0]']    \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_3_bn (BatchNormal  (None, 10, 10, 1024  4096       ['conv4_block6_3_conv[0][0]']    \n",
            " ization)                       )                                                                 \n",
            "                                                                                                  \n",
            " conv4_block6_add (Add)         (None, 10, 10, 1024  0           ['conv4_block5_out[0][0]',       \n",
            "                                )                                 'conv4_block6_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv4_block6_out (Activation)  (None, 10, 10, 1024  0           ['conv4_block6_add[0][0]']       \n",
            "                                )                                                                 \n",
            "                                                                                                  \n",
            " conv5_block1_1_conv (Conv2D)   (None, 5, 5, 512)    524800      ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_1_bn (BatchNormal  (None, 5, 5, 512)   2048        ['conv5_block1_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_1_relu (Activatio  (None, 5, 5, 512)   0           ['conv5_block1_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_2_conv (Conv2D)   (None, 5, 5, 512)    2359808     ['conv5_block1_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_2_bn (BatchNormal  (None, 5, 5, 512)   2048        ['conv5_block1_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_2_relu (Activatio  (None, 5, 5, 512)   0           ['conv5_block1_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block1_0_conv (Conv2D)   (None, 5, 5, 2048)   2099200     ['conv4_block6_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block1_3_conv (Conv2D)   (None, 5, 5, 2048)   1050624     ['conv5_block1_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block1_0_bn (BatchNormal  (None, 5, 5, 2048)  8192        ['conv5_block1_0_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_3_bn (BatchNormal  (None, 5, 5, 2048)  8192        ['conv5_block1_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block1_add (Add)         (None, 5, 5, 2048)   0           ['conv5_block1_0_bn[0][0]',      \n",
            "                                                                  'conv5_block1_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block1_out (Activation)  (None, 5, 5, 2048)   0           ['conv5_block1_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_conv (Conv2D)   (None, 5, 5, 512)    1049088     ['conv5_block1_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block2_1_bn (BatchNormal  (None, 5, 5, 512)   2048        ['conv5_block2_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_1_relu (Activatio  (None, 5, 5, 512)   0           ['conv5_block2_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_2_conv (Conv2D)   (None, 5, 5, 512)    2359808     ['conv5_block2_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_2_bn (BatchNormal  (None, 5, 5, 512)   2048        ['conv5_block2_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_2_relu (Activatio  (None, 5, 5, 512)   0           ['conv5_block2_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block2_3_conv (Conv2D)   (None, 5, 5, 2048)   1050624     ['conv5_block2_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block2_3_bn (BatchNormal  (None, 5, 5, 2048)  8192        ['conv5_block2_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block2_add (Add)         (None, 5, 5, 2048)   0           ['conv5_block1_out[0][0]',       \n",
            "                                                                  'conv5_block2_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block2_out (Activation)  (None, 5, 5, 2048)   0           ['conv5_block2_add[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_conv (Conv2D)   (None, 5, 5, 512)    1049088     ['conv5_block2_out[0][0]']       \n",
            "                                                                                                  \n",
            " conv5_block3_1_bn (BatchNormal  (None, 5, 5, 512)   2048        ['conv5_block3_1_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_1_relu (Activatio  (None, 5, 5, 512)   0           ['conv5_block3_1_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_2_conv (Conv2D)   (None, 5, 5, 512)    2359808     ['conv5_block3_1_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_2_bn (BatchNormal  (None, 5, 5, 512)   2048        ['conv5_block3_2_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_2_relu (Activatio  (None, 5, 5, 512)   0           ['conv5_block3_2_bn[0][0]']      \n",
            " n)                                                                                               \n",
            "                                                                                                  \n",
            " conv5_block3_3_conv (Conv2D)   (None, 5, 5, 2048)   1050624     ['conv5_block3_2_relu[0][0]']    \n",
            "                                                                                                  \n",
            " conv5_block3_3_bn (BatchNormal  (None, 5, 5, 2048)  8192        ['conv5_block3_3_conv[0][0]']    \n",
            " ization)                                                                                         \n",
            "                                                                                                  \n",
            " conv5_block3_add (Add)         (None, 5, 5, 2048)   0           ['conv5_block2_out[0][0]',       \n",
            "                                                                  'conv5_block3_3_bn[0][0]']      \n",
            "                                                                                                  \n",
            " conv5_block3_out (Activation)  (None, 5, 5, 2048)   0           ['conv5_block3_add[0][0]']       \n",
            "                                                                                                  \n",
            " global_average_pooling2d_2 (Gl  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
            " obalAveragePooling2D)                                                                            \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 128)          262272      ['global_average_pooling2d_2[0][0\n",
            "                                                                 ]']                              \n",
            "                                                                                                  \n",
            " dropout_2 (Dropout)            (None, 128)          0           ['dense_4[0][0]']                \n",
            "                                                                                                  \n",
            " dense_5 (Dense)                (None, 1)            129         ['dropout_2[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 23,850,113\n",
            "Trainable params: 262,401\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "  # Instantiate the ImageDataGenerator class \n",
        "  train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
        "                                                      batch_size=10, \n",
        "                                                      class_mode='binary',\n",
        "                                                      target_size=(150, 150))\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class\n",
        "  validation_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
        "                                                                batch_size=10, \n",
        "                                                                class_mode='binary',\n",
        "                                                                target_size=(150, 150))\n",
        "  return train_generator, validation_generator"
      ],
      "metadata": {
        "id": "C-nmUJgdKw68"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = '/content/drive/MyDrive/dataset anjing/bd_final/training'\n",
        "val_dir = '/content/drive/MyDrive/dataset anjing/bd_final/validation'\n",
        "train_generator, validation_generator = train_val_generators(train_dir, val_dir)"
      ],
      "metadata": {
        "id": "cJXg7ur_K-KK",
        "outputId": "7572ac5f-52b0-4eb9-f6f0-58febb83236f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 228 images belonging to 2 classes.\n",
            "Found 58 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_generator,\n",
        "                    validation_data = validation_generator,\n",
        "                    epochs = 10)"
      ],
      "metadata": {
        "id": "uLbavIUbKLyo",
        "outputId": "a8f52e4d-ef84-41bd-9722-f347ad57f263",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "23/23 [==============================] - 25s 951ms/step - loss: 0.7806 - accuracy: 0.4781 - val_loss: 0.7318 - val_accuracy: 0.5000\n",
            "Epoch 2/10\n",
            "23/23 [==============================] - 22s 951ms/step - loss: 0.7610 - accuracy: 0.4474 - val_loss: 0.6998 - val_accuracy: 0.5000\n",
            "Epoch 3/10\n",
            "23/23 [==============================] - 20s 870ms/step - loss: 0.7236 - accuracy: 0.4912 - val_loss: 0.7021 - val_accuracy: 0.5000\n",
            "Epoch 4/10\n",
            "23/23 [==============================] - 21s 913ms/step - loss: 0.7105 - accuracy: 0.5044 - val_loss: 0.6996 - val_accuracy: 0.5000\n",
            "Epoch 5/10\n",
            "23/23 [==============================] - 22s 966ms/step - loss: 0.7018 - accuracy: 0.5307 - val_loss: 0.6985 - val_accuracy: 0.5000\n",
            "Epoch 6/10\n",
            "23/23 [==============================] - 21s 916ms/step - loss: 0.7049 - accuracy: 0.4781 - val_loss: 0.6954 - val_accuracy: 0.5000\n",
            "Epoch 7/10\n",
            "23/23 [==============================] - 21s 928ms/step - loss: 0.6965 - accuracy: 0.4956 - val_loss: 0.6962 - val_accuracy: 0.5000\n",
            "Epoch 8/10\n",
            "23/23 [==============================] - 21s 914ms/step - loss: 0.6923 - accuracy: 0.5351 - val_loss: 0.6957 - val_accuracy: 0.5000\n",
            "Epoch 9/10\n",
            "23/23 [==============================] - 21s 914ms/step - loss: 0.6961 - accuracy: 0.5000 - val_loss: 0.6949 - val_accuracy: 0.4310\n",
            "Epoch 10/10\n",
            "23/23 [==============================] - 22s 961ms/step - loss: 0.6927 - accuracy: 0.5482 - val_loss: 0.6943 - val_accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ARC"
      ],
      "metadata": {
        "id": "LotpC8hIl8Mu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged'\n",
        "\n",
        "if os.path.exists(root_dir):\n",
        "  shutil.rmtree(root_dir)\n",
        "\n",
        "def create_train_val_dirs(root_path):\n",
        "\n",
        "  # List of labels\n",
        "  labels = ['bd', 'fi', 'ha', 'he']\n",
        "\n",
        "  # Create train directory\n",
        "  train_dir = os.path.join(root_path, 'training')\n",
        "  os.makedirs(train_dir, exist_ok=True)\n",
        "\n",
        "  for label in labels:\n",
        "      label_dir = os.path.join(train_dir, label)\n",
        "      os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "  # Create validation directory\n",
        "  val_dir = os.path.join(root_path, 'validation')\n",
        "  os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "  for label in labels:\n",
        "      label_dir = os.path.join(val_dir, label)\n",
        "      os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "  \n",
        "try:\n",
        "  create_train_val_dirs(root_path=root_dir)\n",
        "except FileExistsError:\n",
        "  print(\"Error\")"
      ],
      "metadata": {
        "id": "EXnIKlmPkKZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for rootdir, dirs, files in os.walk(root_dir):\n",
        "    for subdir in dirs:\n",
        "        print(os.path.join(rootdir, subdir))"
      ],
      "metadata": {
        "id": "Y7uAobW1ly2y",
        "outputId": "6a8c76f7-9143-449b-c188-0475d950b637",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/training\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/validation\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/training/bd\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/training/fi\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/training/ha\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/training/he\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/validation/bd\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/validation/fi\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/validation/ha\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/validation/he\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try to Crop"
      ],
      "metadata": {
        "id": "lcCduGe3LO6j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to crop image\n",
        "# note: the amount of croped pixels were approxed by me (nopal) \n",
        "#       by measure the excess pixels\n",
        "def crop_image(image):\n",
        "    crop_width = 1920 - 170 - 150  # Calculate the resulting width after trimming\n",
        "    crop_height = 1080  # Height remains the same\n",
        "    crop_location = (170, 0)  # Starting position of the crop\n",
        "\n",
        "    # Crop the image using array indexing\n",
        "    cropped_image = image[crop_location[1]:crop_location[1] + crop_height,\n",
        "                          crop_location[0]:crop_location[0] + crop_width, :]\n",
        "\n",
        "    return cropped_image"
      ],
      "metadata": {
        "id": "3F37ph3xZkgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to split dataset into training and validation\n",
        "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
        "    files = os.listdir(SOURCE_DIR)\n",
        "\n",
        "    verified_files = []\n",
        "    for file in files:\n",
        "        if os.path.getsize(os.path.join(SOURCE_DIR, file)) == 0:\n",
        "            print(f\"{file} is zero length, so ignoring.\")\n",
        "        else:\n",
        "            verified_files.append(file)\n",
        "\n",
        "    # Shuffle the files\n",
        "    random.shuffle(verified_files)\n",
        "\n",
        "    # Calculate the split index\n",
        "    split_idx = int(SPLIT_SIZE * len(verified_files))\n",
        "\n",
        "    # Split the files\n",
        "    train_files = verified_files[:split_idx]\n",
        "    val_files = verified_files[split_idx:]\n",
        "\n",
        "    # Copy train files\n",
        "    for file in train_files:\n",
        "        src_path = os.path.join(SOURCE_DIR, file)\n",
        "        dst_path = os.path.join(TRAINING_DIR, file)\n",
        "        # Load the image using TensorFlow\n",
        "        image = tf.io.read_file(src_path)\n",
        "        image = tf.image.decode_image(image)\n",
        "        # Crop the image\n",
        "        cropped_image = crop_image(image)\n",
        "        # Save the cropped image to the destination directory\n",
        "        tf.io.write_file(dst_path, tf.image.encode_png(cropped_image))\n",
        "\n",
        "    # Copy validation files\n",
        "    for file in val_files:\n",
        "        src_path = os.path.join(SOURCE_DIR, file)\n",
        "        dst_path = os.path.join(VALIDATION_DIR, file)\n",
        "        # Load the image using TensorFlow\n",
        "        image = tf.io.read_file(src_path)\n",
        "        image = tf.image.decode_image(image)\n",
        "        # Crop the image\n",
        "        cropped_image = crop_image(image)\n",
        "        # Save the cropped image to the destination directory\n",
        "        tf.io.write_file(dst_path, tf.image.encode_png(cropped_image))"
      ],
      "metadata": {
        "id": "8mLeyYK9Kh35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DIR = \"/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/training\"\n",
        "VALIDATION_DIR = \"/content/drive/MyDrive/Datasets/Bangkit - Capstone/normal dataset/arranged/validation\"\n",
        "\n",
        "LABELS = ['bd', 'fi', 'ha', 'he']\n",
        "\n",
        "TRAINING_LABEL_DIRS = [os.path.join(TRAINING_DIR, label) for label in LABELS]\n",
        "VALIDATION_LABEL_DIRS = [os.path.join(VALIDATION_DIR, label) for label in LABELS]\n",
        "\n",
        "# Empty directories in case this cell runs multiple times\n",
        "for label_dir in TRAINING_LABEL_DIRS:\n",
        "    if len(os.listdir(label_dir)) > 0:\n",
        "        for file in os.scandir(label_dir):\n",
        "            os.remove(file.path)\n",
        "\n",
        "for label_dir in VALIDATION_LABEL_DIRS:\n",
        "    if len(os.listdir(label_dir)) > 0:\n",
        "        for file in os.scandir(label_dir):\n",
        "            os.remove(file.path)\n",
        "\n",
        "# Define proportion of images used for training\n",
        "split_size = 0.8\n",
        "\n",
        "# Run the function\n",
        "for label in LABELS:\n",
        "    source_dir = eval(f'{label}_dir')\n",
        "    train_dir = os.path.join(TRAINING_DIR, label)\n",
        "    val_dir = os.path.join(VALIDATION_DIR, label)\n",
        "    split_data(source_dir, train_dir, val_dir, split_size)\n",
        "\n",
        "# Check the number of images in each directory\n",
        "\n",
        "# Original source directories should contain unchanged images\n",
        "for label in LABELS:\n",
        "    source_dir = eval(f'{label}_dir')\n",
        "    print(f\"Original {label}'s directory has {len(os.listdir(source_dir))} images\")\n",
        "\n",
        "# Training and validation splits\n",
        "for i, label in enumerate(LABELS):\n",
        "    print(f\"There are {len(os.listdir(TRAINING_LABEL_DIRS[i]))} images of {label} for training\")\n",
        "    print(f\"There are {len(os.listdir(VALIDATION_LABEL_DIRS[i]))} images of {label} for validation\")"
      ],
      "metadata": {
        "id": "4UFcjWlHrBq1",
        "outputId": "ce2b1008-fa64-4f5a-d907-8a81bb84b990",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original bd's directory has 12 images\n",
            "Original fi's directory has 11 images\n",
            "Original ha's directory has 13 images\n",
            "Original he's directory has 26 images\n",
            "There are 9 images of bd for training\n",
            "There are 3 images of bd for validation\n",
            "There are 8 images of fi for training\n",
            "There are 3 images of fi for validation\n",
            "There are 10 images of ha for training\n",
            "There are 3 images of ha for validation\n",
            "There are 20 images of he for training\n",
            "There are 6 images of he for validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Data Generator"
      ],
      "metadata": {
        "id": "LCm1Ul4OtNT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class and set the desired augmentation parameters\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1.0/255.,\n",
        "      shear_range=0.2,\n",
        "      rotation_range=360,\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "  )\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "      directory=TRAINING_DIR,\n",
        "      batch_size=10,\n",
        "      class_mode='categorical',\n",
        "      target_size=(128, 128)\n",
        "  )\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class and set the rescale parameter\n",
        "  validation_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  validation_generator = validation_datagen.flow_from_directory(\n",
        "      directory=VALIDATION_DIR,\n",
        "      batch_size=10,\n",
        "      class_mode='categorical',\n",
        "      target_size=(128, 128)\n",
        "  )\n",
        "\n",
        "  return train_generator, validation_generator"
      ],
      "metadata": {
        "id": "E4zIT3zmsVnC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the generators\n",
        "train_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)"
      ],
      "metadata": {
        "id": "MMksZeRoCwUH",
        "outputId": "3b7ff462-1ced-427b-ee10-65f3b2aa4725",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 47 images belonging to 4 classes.\n",
            "Found 15 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try Using Restnet"
      ],
      "metadata": {
        "id": "J5dfsgUoE15Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load the pre-trained ResNet50 model without the top layer\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))"
      ],
      "metadata": {
        "id": "p43i5JKJJcBT",
        "outputId": "98e40ff2-3aa7-430f-a6f0-0c70ee3ff62f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "\n",
        "# Create a new model by adding your own classifier on top of the pre-trained ResNet model\n",
        "model = models.Sequential()\n",
        "model.add(base_model)\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(4, activation='softmax'))"
      ],
      "metadata": {
        "id": "Q1Ekw5piJiad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(train_generator, epochs=10, validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "O_QhE1B5JsKm",
        "outputId": "b3b44ca1-aed3-4a27-8cc1-7a9b6c0a55e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "5/5 [==============================] - 46s 5s/step - loss: 8.5143 - accuracy: 0.2766 - val_loss: 15.3162 - val_accuracy: 0.2000\n",
            "Epoch 2/10\n",
            "5/5 [==============================] - 21s 4s/step - loss: 8.8929 - accuracy: 0.1489 - val_loss: 1868.3269 - val_accuracy: 0.2000\n",
            "Epoch 3/10\n",
            "5/5 [==============================] - 21s 4s/step - loss: 7.1826 - accuracy: 0.3617 - val_loss: 822454.6250 - val_accuracy: 0.4000\n",
            "Epoch 4/10\n",
            "5/5 [==============================] - 21s 4s/step - loss: 5.7973 - accuracy: 0.3830 - val_loss: 2531844.0000 - val_accuracy: 0.4000\n",
            "Epoch 5/10\n",
            "5/5 [==============================] - 20s 4s/step - loss: 3.6505 - accuracy: 0.4043 - val_loss: 20459708.0000 - val_accuracy: 0.4000\n",
            "Epoch 6/10\n",
            "5/5 [==============================] - 21s 4s/step - loss: 6.4210 - accuracy: 0.4468 - val_loss: 9382912.0000 - val_accuracy: 0.4000\n",
            "Epoch 7/10\n",
            "5/5 [==============================] - 20s 4s/step - loss: 2.6669 - accuracy: 0.4681 - val_loss: 1328487.7500 - val_accuracy: 0.2000\n",
            "Epoch 8/10\n",
            "5/5 [==============================] - 20s 4s/step - loss: 2.5596 - accuracy: 0.4043 - val_loss: 2256.4392 - val_accuracy: 0.2000\n",
            "Epoch 9/10\n",
            "5/5 [==============================] - 21s 4s/step - loss: 3.0563 - accuracy: 0.4255 - val_loss: 396.8612 - val_accuracy: 0.2000\n",
            "Epoch 10/10\n",
            "5/5 [==============================] - 21s 4s/step - loss: 3.5707 - accuracy: 0.4894 - val_loss: 87.5492 - val_accuracy: 0.2000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelling"
      ],
      "metadata": {
        "id": "VoYIL11bEu-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([ \n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "                loss=tf.keras.losses.binary_crossentropy,\n",
        "                metrics=['accuracy']) \n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "e83EyCP3DE6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=100,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "gu69sMw2DtuR",
        "outputId": "d3d7f092-9a60-4b44-8017-57cca80f9b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "50/50 [==============================] - 83s 2s/step - loss: 0.5686 - accuracy: 0.3972 - val_loss: 0.5436 - val_accuracy: 0.4160\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 93s 2s/step - loss: 0.5454 - accuracy: 0.4194 - val_loss: 0.5413 - val_accuracy: 0.4160\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 74s 1s/step - loss: 0.5435 - accuracy: 0.4194 - val_loss: 0.5408 - val_accuracy: 0.4160\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 73s 1s/step - loss: 0.5421 - accuracy: 0.4194 - val_loss: 0.5398 - val_accuracy: 0.4160\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 75s 1s/step - loss: 0.5420 - accuracy: 0.4194 - val_loss: 0.5401 - val_accuracy: 0.4160\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 83s 2s/step - loss: 0.5408 - accuracy: 0.4194 - val_loss: 0.5407 - val_accuracy: 0.4160\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 83s 2s/step - loss: 0.5420 - accuracy: 0.4194 - val_loss: 0.5392 - val_accuracy: 0.4160\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 80s 2s/step - loss: 0.5411 - accuracy: 0.4194 - val_loss: 0.5394 - val_accuracy: 0.4160\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 74s 1s/step - loss: 0.5411 - accuracy: 0.4194 - val_loss: 0.5402 - val_accuracy: 0.4160\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 74s 1s/step - loss: 0.5416 - accuracy: 0.4194 - val_loss: 0.5399 - val_accuracy: 0.4160\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 75s 1s/step - loss: 0.5410 - accuracy: 0.4194 - val_loss: 0.5400 - val_accuracy: 0.4160\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 74s 1s/step - loss: 0.5410 - accuracy: 0.4194 - val_loss: 0.5395 - val_accuracy: 0.4160\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 73s 1s/step - loss: 0.5405 - accuracy: 0.4194 - val_loss: 0.5396 - val_accuracy: 0.4160\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 84s 2s/step - loss: 0.5420 - accuracy: 0.4194 - val_loss: 0.5404 - val_accuracy: 0.4160\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 73s 1s/step - loss: 0.5403 - accuracy: 0.4194 - val_loss: 0.5486 - val_accuracy: 0.4160\n",
            "Epoch 16/100\n",
            "40/50 [=======================>......] - ETA: 12s - loss: 0.5428 - accuracy: 0.4141"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-bdec714a1ef8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(train_generator,\n\u001b[0m\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Try to crop"
      ],
      "metadata": {
        "id": "DogHZQ3MGnwY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jNjBfGgFE1UJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}