{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOQIoaRZdZJVCSzDg98gITx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C23-PS435-bangkit/MachineLearning/blob/main/Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libs"
      ],
      "metadata": {
        "id": "me6oML2PklmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "uJzZqF4Dkt3X"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "TQN4Fe4mAyWj",
        "outputId": "212043c8-8053-400a-b259-c192eaca68b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Split Datasets"
      ],
      "metadata": {
        "id": "8XGjuWNYkqh-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base_dir = '/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset'\n",
        "\n",
        "bd_dir = os.path.join(base_dir, 'bacterial_dermatosis')\n",
        "fi_dir = os.path.join(base_dir, 'fungal_infection')\n",
        "ha_dir = os.path.join(base_dir, 'hypersensitivity_allergic_dermatosis')\n",
        "he_dir = os.path.join(base_dir, 'healthy')\n",
        "\n",
        "print('total training bacterial_dermatosis images:', len(os.listdir(bd_dir)))\n",
        "print('total training fungal_infection images:', len(os.listdir(fi_dir)))\n",
        "print('total training hypersensitivity_allergic_dermatosis images:', len(os.listdir(ha_dir)))\n",
        "print('total training healthy images:', len(os.listdir(he_dir)))\n",
        "\n",
        "bd_files = os.listdir(bd_dir)\n",
        "print(bd_files[:10])\n",
        "\n",
        "fi_files = os.listdir(fi_dir)\n",
        "print(fi_files[:10])\n",
        "\n",
        "ha_files = os.listdir(ha_dir)\n",
        "print(ha_files[:10])\n",
        "\n",
        "he_files = os.listdir(he_dir)\n",
        "print(he_files[:10])"
      ],
      "metadata": {
        "id": "3s9Oe0YOC9zr",
        "outputId": "c9ff2edc-3aa4-404c-9c2b-de83d6dd77f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total training bacterial_dermatosis images: 121\n",
            "total training fungal_infection images: 110\n",
            "total training hypersensitivity_allergic_dermatosis images: 130\n",
            "total training healthy images: 260\n",
            "['dog210612_03_01_13_pic1.jpg', 'dog210612_03_01_13_pic6.jpg', 'dog210612_03_01_13_pic0.jpg', 'dog210612_03_01_13_pic5.jpg', 'dog210612_03_01_13_pic3.jpg', 'dog210612_03_01_13_pic7.jpg', 'dog210612_03_01_13_pic9.jpg', 'dog210612_03_01_13_pic8.jpg', 'dog210612_03_01_13_pic2.jpg', 'dog210612_03_01_13_pic4.jpg']\n",
            "['dog210422_04_02_33_pic7.jpg', 'dog210422_04_02_33_pic0.jpg', 'dog210422_04_02_33_pic6.jpg', 'dog210422_04_02_33_pic8.jpg', 'dog210422_04_02_33_pic5.jpg', 'dog210422_04_02_33_pic9.jpg', 'dog210422_04_02_33_pic4.jpg', 'dog210422_04_02_33_pic2.jpg', 'dog210422_04_02_33_pic3.jpg', 'dog210422_04_02_33_pic1.jpg']\n",
            "['dog210424_05_01_31_pic0.jpg', 'dog210424_05_01_31_pic6.jpg', 'dog210424_05_01_31_pic1.jpg', 'dog210424_05_01_31_pic8.jpg', 'dog210424_05_01_31_pic4.jpg', 'dog210424_05_01_31_pic2.jpg', 'dog210424_05_01_31_pic3.jpg', 'dog210424_05_01_31_pic9.jpg', 'dog210424_05_01_31_pic5.jpg', 'dog210424_05_01_31_pic7.jpg']\n",
            "['dog210505_01_02_26_pic3.jpg', 'dog210505_01_02_25_pic2.jpg', 'dog210505_01_02_26_pic2.jpg', 'dog210505_01_02_25_pic8.jpg', 'dog210505_01_02_25_pic6.jpg', 'dog210505_01_02_25_pic0.jpg', 'dog210505_01_02_25_pic4.jpg', 'dog210505_01_02_26_pic0.jpg', 'dog210505_01_02_25_pic3.jpg', 'dog210505_01_02_25_pic7.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "root_dir = '/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged'\n",
        "\n",
        "if os.path.exists(root_dir):\n",
        "  shutil.rmtree(root_dir)\n",
        "\n",
        "def create_train_val_dirs(root_path):\n",
        "\n",
        "  # List of labels\n",
        "  labels = ['bd', 'fi', 'ha', 'he']\n",
        "\n",
        "  # Create train directory\n",
        "  train_dir = os.path.join(root_path, 'training')\n",
        "  os.makedirs(train_dir, exist_ok=True)\n",
        "\n",
        "  for label in labels:\n",
        "      label_dir = os.path.join(train_dir, label)\n",
        "      os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "  # Create validation directory\n",
        "  val_dir = os.path.join(root_path, 'validation')\n",
        "  os.makedirs(val_dir, exist_ok=True)\n",
        "\n",
        "  for label in labels:\n",
        "      label_dir = os.path.join(val_dir, label)\n",
        "      os.makedirs(label_dir, exist_ok=True)\n",
        "\n",
        "  \n",
        "try:\n",
        "  create_train_val_dirs(root_path=root_dir)\n",
        "except FileExistsError:\n",
        "  print(\"Error\")"
      ],
      "metadata": {
        "id": "EXnIKlmPkKZe"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for rootdir, dirs, files in os.walk(root_dir):\n",
        "    for subdir in dirs:\n",
        "        print(os.path.join(rootdir, subdir))"
      ],
      "metadata": {
        "id": "Y7uAobW1ly2y",
        "outputId": "4a5148b5-6a93-4ce0-f7ad-c8ba04534a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/training\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/validation\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/training/bd\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/training/fi\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/training/ha\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/training/he\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/validation/bd\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/validation/fi\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/validation/ha\n",
            "/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/validation/he\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(SOURCE_DIR, TRAINING_DIR, VALIDATION_DIR, SPLIT_SIZE):\n",
        "  files = os.listdir(SOURCE_DIR)\n",
        "\n",
        "  verified_files = []\n",
        "  for file in files :\n",
        "    if os.path.getsize(os.path.join(SOURCE_DIR, file)) == 0 :\n",
        "      print(f\"{file} is zero length, so ignoring.\")\n",
        "    else :\n",
        "      verified_files.append(file)\n",
        "      \n",
        "\n",
        "  # shuffle the files\n",
        "  random.shuffle(verified_files)\n",
        "\n",
        "  # calculate the split index\n",
        "  split_idx = int(SPLIT_SIZE * len(verified_files))\n",
        "\n",
        "  # split the files\n",
        "  train_files = verified_files[:split_idx]\n",
        "  val_files = verified_files[split_idx:]\n",
        "\n",
        "  # copy train files\n",
        "  for file in train_files:\n",
        "      src_path = os.path.join(SOURCE_DIR, file)\n",
        "      dst_path = os.path.join(TRAINING_DIR, file)\n",
        "      shutil.copy(src_path, dst_path)\n",
        "\n",
        "  # copy validation files\n",
        "  for file in val_files:\n",
        "      src_path = os.path.join(SOURCE_DIR, file)\n",
        "      dst_path = os.path.join(VALIDATION_DIR, file)\n",
        "      shutil.copy(src_path, dst_path)"
      ],
      "metadata": {
        "id": "CkkNobqBl-fh"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DIR = \"/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/training/\"\n",
        "VALIDATION_DIR = \"/content/drive/MyDrive/Datasets/Bangkit - Capstone/dataset/arranged/validation/\"\n",
        "\n",
        "LABELS = ['bd', 'fi', 'ha', 'he']\n",
        "\n",
        "TRAINING_LABEL_DIRS = [os.path.join(TRAINING_DIR, label) for label in LABELS]\n",
        "VALIDATION_LABEL_DIRS = [os.path.join(VALIDATION_DIR, label) for label in LABELS]\n",
        "\n",
        "# Empty directories in case this cell runs multiple times\n",
        "for label_dir in TRAINING_LABEL_DIRS:\n",
        "    if len(os.listdir(label_dir)) > 0:\n",
        "        for file in os.scandir(label_dir):\n",
        "            os.remove(file.path)\n",
        "\n",
        "for label_dir in VALIDATION_LABEL_DIRS:\n",
        "    if len(os.listdir(label_dir)) > 0:\n",
        "        for file in os.scandir(label_dir):\n",
        "            os.remove(file.path)\n",
        "\n",
        "# Define proportion of images used for training\n",
        "split_size = 0.8\n",
        "\n",
        "# Run the function\n",
        "for label in LABELS:\n",
        "    source_dir = eval(f'{label}_dir')\n",
        "    train_dir = os.path.join(TRAINING_DIR, label)\n",
        "    val_dir = os.path.join(VALIDATION_DIR, label)\n",
        "    split_data(source_dir, train_dir, val_dir, split_size)\n",
        "\n",
        "# Check the number of images in each directory\n",
        "\n",
        "# Original source directories should contain unchanged images\n",
        "for label in LABELS:\n",
        "    source_dir = eval(f'{label}_dir')\n",
        "    print(f\"Original {label}'s directory has {len(os.listdir(source_dir))} images\")\n",
        "\n",
        "# Training and validation splits\n",
        "for i, label in enumerate(LABELS):\n",
        "    print(f\"There are {len(os.listdir(TRAINING_LABEL_DIRS[i]))} images of {label} for training\")\n",
        "    print(f\"There are {len(os.listdir(VALIDATION_LABEL_DIRS[i]))} images of {label} for validation\")"
      ],
      "metadata": {
        "id": "4UFcjWlHrBq1",
        "outputId": "5aee7872-e993-4844-bb68-ab586f3d499a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original bd's directory has 121 images\n",
            "Original fi's directory has 110 images\n",
            "Original ha's directory has 130 images\n",
            "Original he's directory has 260 images\n",
            "There are 96 images of bd for training\n",
            "There are 25 images of bd for validation\n",
            "There are 88 images of fi for training\n",
            "There are 22 images of fi for validation\n",
            "There are 104 images of ha for training\n",
            "There are 26 images of ha for validation\n",
            "There are 208 images of he for training\n",
            "There are 52 images of he for validation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Data Generator"
      ],
      "metadata": {
        "id": "LCm1Ul4OtNT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class and set the desired augmentation parameters\n",
        "  train_datagen = ImageDataGenerator(\n",
        "      rescale=1.0/255.,\n",
        "      shear_range=0.2,\n",
        "      rotation_range=360,\n",
        "      horizontal_flip=True,\n",
        "      vertical_flip=True\n",
        "  )\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  train_generator = train_datagen.flow_from_directory(\n",
        "      directory=TRAINING_DIR,\n",
        "      batch_size=10,\n",
        "      class_mode='categorical',\n",
        "      target_size=(128, 128)\n",
        "  )\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class and set the rescale parameter\n",
        "  validation_datagen = ImageDataGenerator(rescale=1.0/255.)\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  validation_generator = validation_datagen.flow_from_directory(\n",
        "      directory=VALIDATION_DIR,\n",
        "      batch_size=10,\n",
        "      class_mode='categorical',\n",
        "      target_size=(128, 128)\n",
        "  )\n",
        "\n",
        "  return train_generator, validation_generator"
      ],
      "metadata": {
        "id": "E4zIT3zmsVnC"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the generators\n",
        "train_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)"
      ],
      "metadata": {
        "id": "MMksZeRoCwUH",
        "outputId": "92bd4c1c-6a69-442b-b770-f7fb4aa40b67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 496 images belonging to 4 classes.\n",
            "Found 125 images belonging to 4 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "  model = tf.keras.models.Sequential([ \n",
        "    # This is the first convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(128, 128, 3)),\n",
        "    tf.keras.layers.MaxPooling2D(2, 2),\n",
        "    # The second convolution\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The third convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # The fourth convolution\n",
        "    tf.keras.layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    # Flatten the results to feed into a DNN\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    # 512 neuron hidden layer\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\n",
        "    tf.keras.layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "  \n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
        "                loss=tf.keras.losses.binary_crossentropy,\n",
        "                metrics=['accuracy']) \n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "e83EyCP3DE6M"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model()\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=100,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "gu69sMw2DtuR",
        "outputId": "d3d7f092-9a60-4b44-8017-57cca80f9b2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 937
        }
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "50/50 [==============================] - 83s 2s/step - loss: 0.5686 - accuracy: 0.3972 - val_loss: 0.5436 - val_accuracy: 0.4160\n",
            "Epoch 2/100\n",
            "50/50 [==============================] - 93s 2s/step - loss: 0.5454 - accuracy: 0.4194 - val_loss: 0.5413 - val_accuracy: 0.4160\n",
            "Epoch 3/100\n",
            "50/50 [==============================] - 74s 1s/step - loss: 0.5435 - accuracy: 0.4194 - val_loss: 0.5408 - val_accuracy: 0.4160\n",
            "Epoch 4/100\n",
            "50/50 [==============================] - 73s 1s/step - loss: 0.5421 - accuracy: 0.4194 - val_loss: 0.5398 - val_accuracy: 0.4160\n",
            "Epoch 5/100\n",
            "50/50 [==============================] - 75s 1s/step - loss: 0.5420 - accuracy: 0.4194 - val_loss: 0.5401 - val_accuracy: 0.4160\n",
            "Epoch 6/100\n",
            "50/50 [==============================] - 83s 2s/step - loss: 0.5408 - accuracy: 0.4194 - val_loss: 0.5407 - val_accuracy: 0.4160\n",
            "Epoch 7/100\n",
            "50/50 [==============================] - 83s 2s/step - loss: 0.5420 - accuracy: 0.4194 - val_loss: 0.5392 - val_accuracy: 0.4160\n",
            "Epoch 8/100\n",
            "50/50 [==============================] - 80s 2s/step - loss: 0.5411 - accuracy: 0.4194 - val_loss: 0.5394 - val_accuracy: 0.4160\n",
            "Epoch 9/100\n",
            "50/50 [==============================] - 74s 1s/step - loss: 0.5411 - accuracy: 0.4194 - val_loss: 0.5402 - val_accuracy: 0.4160\n",
            "Epoch 10/100\n",
            "50/50 [==============================] - 74s 1s/step - loss: 0.5416 - accuracy: 0.4194 - val_loss: 0.5399 - val_accuracy: 0.4160\n",
            "Epoch 11/100\n",
            "50/50 [==============================] - 75s 1s/step - loss: 0.5410 - accuracy: 0.4194 - val_loss: 0.5400 - val_accuracy: 0.4160\n",
            "Epoch 12/100\n",
            "50/50 [==============================] - 74s 1s/step - loss: 0.5410 - accuracy: 0.4194 - val_loss: 0.5395 - val_accuracy: 0.4160\n",
            "Epoch 13/100\n",
            "50/50 [==============================] - 73s 1s/step - loss: 0.5405 - accuracy: 0.4194 - val_loss: 0.5396 - val_accuracy: 0.4160\n",
            "Epoch 14/100\n",
            "50/50 [==============================] - 84s 2s/step - loss: 0.5420 - accuracy: 0.4194 - val_loss: 0.5404 - val_accuracy: 0.4160\n",
            "Epoch 15/100\n",
            "50/50 [==============================] - 73s 1s/step - loss: 0.5403 - accuracy: 0.4194 - val_loss: 0.5486 - val_accuracy: 0.4160\n",
            "Epoch 16/100\n",
            "40/50 [=======================>......] - ETA: 12s - loss: 0.5428 - accuracy: 0.4141"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-bdec714a1ef8>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m history = model.fit(train_generator,\n\u001b[0m\u001b[1;32m      4\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}